{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNk+00rEmZMZtNS9CVXAK16"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1. 벡터 데이터베이스**\n","벡터 데이터베이스(Vector Database)는 텍스트, 이미지, 오디오와 같은 데이터를 고차원 벡터 형태로 변환해 저장하고, 이 벡터 간의 유사도를 빠르게 검색할 수 있도록 최적화된 데이터베이스입니다. 일반적인 관계형 데이터베이스가 정확한 값 기반 검색(SQL 쿼리 등)에 적합하다면, 벡터 데이터베이스는 의미적 유사성(semantic similarity)에 기반한 검색을 지원하여 예를 들어 \"강아지\"와 \"개\"처럼 다른 표현이라도 비슷한 의미의 데이터를 찾아낼 수 있습니다. 이를 위해 코사인 유사도, 내적(dot product), 유클리드 거리와 같은 수학적 거리 계산을 활용하며, 대규모 임베딩(embedding) 데이터를 효율적으로 관리하고 검색할 수 있어 추천 시스템, 검색 엔진, 생성형 AI의 RAG(Retrieval-Augmented Generation) 등에 널리 활용됩니다."],"metadata":{"id":"Gz0d_vNvBOCc"}},{"cell_type":"markdown","source":["**크로마디비**\n","\n","[크로마디비(ChromaDB)](https://www.trychroma.com/)는 대표적인 오픈소스 벡터 데이터베이스로, 문서·이미지·코드 등 다양한 데이터를 임베딩 벡터로 변환해 저장하고, 이를 빠르게 검색할 수 있도록 설계된 시스템입니다. 파이썬 기반으로 사용이 간편하며, LangChain 같은 LLM 프레임워크와 잘 통합되어 RAG(Retrieval-Augmented Generation) 구조를 쉽게 구축할 수 있습니다. 내부적으로는 벡터 인덱싱과 메타데이터 저장을 함께 지원하여, 단순히 유사도 검색뿐 아니라 조건 필터링과 결합된 검색도 가능합니다. 무료로 가볍게 실행할 수 있고, 로컬 환경부터 클라우드까지 유연하게 확장할 수 있어 학습용이나 실무용 AI 검색 엔진 구축에 많이 활용됩니다."],"metadata":{"id":"IEq4tYZKBbMW"}},{"cell_type":"code","source":["import getpass\n","import os\n","\n","def _set_env(var: str):\n","    if not os.environ.get(var):\n","        os.environ[var] = getpass.getpass(f\"{var}: \")\n","\n","_set_env(\"OPENAI_API_KEY\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fb7bfHhxBduK","executionInfo":{"status":"ok","timestamp":1758070105827,"user_tz":-540,"elapsed":3537,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"4faf34a7-2fff-469c-e062-5df72cd795f5"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["OPENAI_API_KEY: ··········\n"]}]},{"cell_type":"code","source":["!pip install langchain_community\n","\n","!pip install langchain_experimental\n","\n","!pip install langchain_openai\n","\n","!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"kZwdmIFlBgih","executionInfo":{"status":"ok","timestamp":1758070144857,"user_tz":-540,"elapsed":39026,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"ede0095a-8a41-4f5e-c5ce-f604752530e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n","Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.76)\n","Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.27)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.43)\n","Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.10.1)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.27)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.24.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2025.8.3)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n","Requirement already satisfied: langchain_experimental in /usr/local/lib/python3.12/dist-packages (0.3.4)\n","Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_experimental) (0.3.29)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.12/dist-packages (from langchain_experimental) (0.3.76)\n","Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.27)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.43)\n","Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.5)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.12.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.10.1)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.27)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (25.0)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.11.7)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.11)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.24.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.4.1)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2025.8.3)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.2.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (4.10.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.16.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n","Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (0.3.33)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.3.76)\n","Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.107.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.27)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (6.0.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (25.0)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.11.7)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.67.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.5)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.24.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n","Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n"]}]},{"cell_type":"markdown","source":["# **2. 청크**\n","청크(Chunk)는 긴 텍스트나 문서를 작은 단위로 나눈 조각을 의미하며, 주로 자연어 처리와 RAG(Retrieval-Augmented Generation) 같은 작업에서 사용됩니다. 대형 언어모델은 한 번에 처리할 수 있는 토큰 수에 한계가 있기 때문에 문서를 일정한 길이로 분할하여 임베딩 벡터로 변환하고, 이후 검색이나 질의 응답 시 필요한 청크만 불러와 모델에 전달하는 방식으로 효율성과 정확성을 높입니다. 청크는 단순히 일정 글자 수나 토큰 수로 나누기도 하지만, 문단·문장 단위 등 의미 단위로 나누어야 검색 품질이 좋아지며, 결국 청크는 방대한 데이터를 모델이 다룰 수 있는 크기로 잘게 나눈 최소 단위라고 할 수 있습니다."],"metadata":{"id":"eZFkLcHrBhp8"}},{"cell_type":"markdown","source":["**SemanticChunker**\n","\n","SemanticChunker는 텍스트를 단순히 일정한 길이로 자르는 방식이 아니라, 문장의 의미적 맥락을 고려해 자연스럽게 분할하는 청크 생성 기법입니다. 즉, 문장이나 문단의 의미가 단절되지 않도록 문맥 단위로 텍스트를 나누어 임베딩과 검색의 정확도를 높여줍니다. 이를 통해 RAG(Retrieval-Augmented Generation) 구조에서 모델이 보다 관련성 높은 정보를 검색할 수 있으며, 불필요하게 잘려나간 조각이나 중복된 정보 전달을 줄일 수 있습니다. 따라서 SemanticChunker는 의미 기반 검색과 대규모 문서 처리에서 효율성과 정밀도를 동시에 향상시키는 중요한 도구로 활용됩니다."],"metadata":{"id":"Ndh3fUgSBjU0"}},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFLoader\n","from langchain_experimental.text_splitter import SemanticChunker\n","from langchain_openai.embeddings import OpenAIEmbeddings"],"metadata":{"id":"uZABCYohBmSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path = \"SPRi AI Brief_4월호_산업동향_250407_F.pdf\""],"metadata":{"id":"TwjdYkGVERvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = PyPDFLoader(file_path)\n","pages = []\n","\n","# 비동기로 한 번에 전부 읽지 않고 페이지 단위로 스트리밍\n","async for page in loader.alazy_load():\n","    pages.append(page)\n","\n","text_splitter = SemanticChunker(OpenAIEmbeddings())\n","\n","# 페이지별 문서를 여러 개의 의미 청크로 쪼개 반환\n","docs = text_splitter.split_documents(pages)\n","\n","print(f\"총 {len(docs)}개 만큼의 문서로 청킹되었습니다.\")\n","print([len(i.page_content) for i in docs])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ar9PkCjZEZsi","executionInfo":{"status":"ok","timestamp":1758070167927,"user_tz":-540,"elapsed":11533,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"eac3207a-d033-4f43-c2fd-a16f970fc32c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["총 33개 만큼의 문서로 청킹되었습니다.\n","[22, 922, 19, 1562, 1447, 1645, 1682, 19, 1291, 1777, 1801, 1528, 1656, 1755, 78, 1659, 1055, 19, 1545, 1662, 1690, 1628, 91, 1684, 19, 1619, 10, 1447, 1678, 58, 1616, 2460, 225]\n"]}]},{"cell_type":"code","source":["# 각 청크의 메타데이터 및 내용 출력\n","for i in docs:\n","    print(i.metadata)       # 문서의 메타데이터 출력 (예: 페이지 번호 등)\n","    print(i.page_content)   # 분할된 청크의 내용 출력\n","    print(\"-\" * 100)        # 구분선 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"uQupx547EbYw","executionInfo":{"status":"ok","timestamp":1758070168434,"user_tz":-540,"elapsed":505,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"68a7993d-4dd6-45eb-8870-e67eb8a533ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1'}\n","2025년4월호인공지능 산업의 최신 동향\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2'}\n","SPRi AI Brief2025년 4월호\n","2\n","CONTENTS정책･법제∙2025년 중국 양회의 정부 업무보고, AI 플러스 행동의 지속 추진을 천명2∙중국 정부, 2025년 9월부터 AI 생성·합성물 표시 의무화 규정 시행 예정 발표3∙인도 전자정보기술부, AI 강국 실현을 위한 AI 로드맵 공개4∙영국 경쟁시장청, 마이크로소프트와 오픈AI 협력 관련 반독점 조사 종료 발표5기업･산업∙구글, 단일 GPU로 구동 가능한 오픈소스 AI 모델 ‘젬마 3’ 공개7∙ 오픈AI, AI 에이전트 구축 지원 도구와 신규 음성·이미지 생성 모델 출시8∙앤스로픽, 트럼프 행정부에 국가 경제와 안보를 고려한 AI 정책 제안9∙ 오픈AI, 트럼프 행정부에 혁신의 자유 강조하는 AI 정책 제안10∙엔비디아, GTC 2025에서 신규 AI 칩 로드맵과 AI 제품군 공개11∙마누스 AI, 완전 자율 AI 에이전트 ‘마누스’ 공개12∙피규어, 휴머노이드 로봇 제조시설 ‘BotQ’ 발표 13∙앱트로닉, 자빌과 전략적 제휴로 휴머노이드 로봇 제작 추진14기술･연구∙구글 딥마인드, 로봇 특화 AI 모델 ‘제미나이 로보틱스’ 개발16∙사카나 AI의 AI 생성 논문, ICLR 2025 워크숍에서 동료 심사 통과17∙AAAI, 주요 AI 연구 주제와 해결 과제를 정리한 보고서 발간18∙2024년 튜링상, 강화학습 연구에 기여한 연구자 2인이 수상19∙카카오, 자체 개발 AI 모델 ‘카나나’의 테크니컬 리포트 공개20인력･교육 ∙영국 옥스퍼드⼤ 연구 결과, AI 인력 채용 시 학위보다 실무기술이 중요22∙스탠포드 HAI, AI+교육 서밋에서 AI가 교육에 미치는 영향 논의23∙영국 정부의 AI 저작권 규제 완화 기조에 창작자들의 반발 격화24∙IBM CEO, 가까운 미래에 AI가 프로그래머를 대체할 가능성은 희박하다고 예측25주요행사일정26\n","| 2025년 4월호 |\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3'}\n","| 2025년 4월호 |\n","정책･법제\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4'}\n","SPRi AI Brief2025년 4월호\n","2\n","2025년 중국 양회의 정부 업무보고, AI 플러스 행동의 지속 추진을 천명n중국 정부는 2025년 양회의 정부 업무보고를 통해 2024년 처음 제시한 ‘AI 플러스’ 행동을 지속 추진하고 대규모 AI 모델의 광범위한 적용을 지원하겠다는 계획을 발표n2025년 정부 업무보고는 AI 휴대폰과 컴퓨터, 스마트 로봇 등의 차세대 스마트 단말의 개발과 체화 지능의 중요성도 강조했으며, 양회에 참석한 각계 대표들도 AI와 관련된 다양한 제안을 제출 \n","KEY Contents\n","£중국 정부, ‘AI 플러스’ 행동의 지속 추진 및 대규모 AI 모델의 도입 지원 계획n중국 정부는 2025년 3월 5일 열린 양회(兩會)* 전국인민대표대회 개막식에서 발표한 2025년 정부 업무보고에서 ‘AI 플러스’ 행동의 지속 추진을 천명* 입법기관인 전국인민대표대회(전인대)와 최고 정책 자문기구인 전국인민정치협상회의(정협)∙중국 정부는 2024년 업무보고에서 AI와 실물경제의 통합을 강화하기 위한 ‘AI 플러스’ 개념을 처음 제시했으며, 올해 정부 과제로 디지털 경제 활성화를 위해 “AI 플러스 행동을 지속 추진하고 시장과 제조업에서 디지털 기술의 결합을 강화하며 대규모 AI 모델의 광범위한 적용을 지원”할 계획∙스마트 커넥티드 차량, AI 휴대폰과 컴퓨터, 스마트 로봇, 스마트 제조 장비 등 차세대 스마트 단말을 적극 개발하겠다는 계획도 제시∙업무보고는 또한 ‘신흥산업과 미래산업의 육성과 발전’ 항목에서 미래산업으로 바이오 제조, 양자 기술, 6G와 함께 체화 지능(Embodied Intelligence)*을 포함* 물리적 실체를 갖고 실제 환경과 상호작용하는 AI£양회에 참석한 각계 대표들, AI와 관련된 다양한 제안 제출n중국 관영 언론 글로벌 타임스(Global Times)에 따르면 정부 업무보고에 체화 지능과 대규모 AI 모델이 언급된 것은 올해가 처음이며, 양회에 참석한 각계 대표들도 AI와 관련된 다양한 제안을 제출하는 등, AI가 양회의 핵심 화두로 부상∙정협 위원인 류샹시(劉尚希) 중국 재정과학연구원 전임 원장은 높은 비용과 막대한 투자가 AI 접근성을 제한해 왔다며, 딥시크(DeepSeek) 같은 오픈소스 AI 모델이 AI 접근을 민주화할 것이라고 평가∙정협 위원인 주송춘(朱松纯) 베이징범용AI연구소 원장은 AI 연구소 설립을 추진하는 대학들이 역량 있는 교수진 모집에 어려움을 겪고 있다며 산업의 수요 증가에 대응해 AI 전문가를 지원하는 방안을 제출하겠다고 발언 ∙레이쥔(雷軍) 샤오미(Xiaomi) CEO 겸 전인대 대표는 AI 기반 얼굴 합성과 음성 복제 기술의 남용 억제를 강조하는 한편, 업계의 자정 노력 강화와 AI에 대한 대중 인식 제고를 촉구∙정협 위원인 리징훙(李景虹) 칭화대(清华⼤) 교수는 인재 양성과 고용 보장, 국제 협력, 지식재산권 등 광범위한 AI 문제를 포괄하는 AI 진흥법 도입을 제안출처 | 中国政府网, 最全！50个动态场景看2025《政府⼯作报告》全⽂, 2025.03.05. Global Times, AI in spotlight at China's \"two sessions\", 2025.03.09.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5'}\n","정책･법제기업･산업기술･연구인력･교육\n","3\n","중국 정부, 2025년 9월부터 AI 생성·합성물 표시 의무화 규정 시행 예정 발표n중국 국가인터넷정보판공실이 AI 생성·합성물에 명시적 또는 암묵적 표시를 의무화하는 규정을 발표하고 2025년 9월 1일부터 시행할 예정n규정은 대중의 혼란이나 오해를 유발할 수 있는 AI 생성·합성물에는 명시적 표시를 추가하고, 서비스제공자에게 AI 생성·합성물의 파일 메타데이터에 암묵적 표시를 추가할 것을 요구\n","KEY Contents\n","£대중의 오해나 착각을 유발할 수 있는 AI 생성· 합성물에 명시적 표시 의무 적용n중국 국가인터넷정보판공실(CAC)이 2025년 3월 14일 공업정보화부, 공안부, 국가광전총국과 공동으로 인터넷 정보 서비스 사업자를 대상으로 AI 생성·합성물에 명시적 또는 암묵적 표시를 의무화하는 「AI 생성·합성 콘텐츠 표시 방법」 규정을 발표하고 2025년 9월 1일부터 시행∙규정에 따르면 AI 생성·합성물에 대한 표시는 사용자가 명확히 인식할 수 있는 명시적 표시와, 파일의 메타데이터에 추가되어 사용자가 쉽게 인식할 수 없는 암묵적 표시로 구분 n인터넷 정보 서비스 사업자가 사람의 텍스트 생성이나 편집을 모방한 지능형 대화나 작문과 같이 대중의 혼돈이나 착각을 유발할 수 있는 AI 생성·합성물*을 제공 시에는 명시적 표시를 추가*텍스트, 오디오, 이미지, 동영상, 가상 장면 등∙일례로 텍스트의 시작, 끝, 또는 중간의 적절한 위치에 텍스트나 일반 기호로 된 표시를 추가하거나 상호작용 가능한 사용자 인터페이스나 텍스트 주변에 명시적 표시를 추가∙AI 생성·합성물 서비스 제공자가 해당 콘텐츠를 다운로드, 복사, 내보내기 등의 방식으로 제공할 경우 명시적 표시가 포함되어 있는지 확인 필요n인터넷 정보 서비스 사업자가 AI 생성·합성물 서비스 제공 시에는 파일 메타데이터*에 △콘텐츠 속성 △서비스 제공자의 명칭이나 코드 △콘텐츠 번호를 포함하도록 디지털 워터마크와 같은 암묵적 표시를 추가* 파일의 출처, 속성, 용도 등 파일에 관한 정보를 담은 데이터n온라인 정보 콘텐츠를 보급하는 서비스 제공자는 AI 생성·합성물 제공 시 다음과 같은 조치를 시행∙파일 메타데이터에 암묵적 표시가 있으면 해당 콘텐츠가 AI 생성·합성물임을 나타내는 명시적 표시를 추가하고, 파일 메타데이터에 암묵적 표시가 없으나 사용자가 AI 생성·합성물임을 밝힌 경우에는 게시된 콘텐츠 주위에 명시적 표시를 추가∙파일 메타데이터에 암묵적 표시가 없고 사용자가 AI  생성·합성물이라고 밝히지 않았으나 AI 생성이나 합성의 다른 흔적을 감지한 경우, AI 생성·합성물로 의심된다는 명확한 경고 표시를 추가 필요n동 규정에 따른 AI 생성·합성물 표시에 대한 악의적 삭제, 조작, 은폐는 금지되며, 이러한 행위를 할 수 있는 도구나 서비스의 제공도 금지 출처 | 国家互联网信息办公室, 关于印发《⼈⼯智能⽣成合成内容标识办法》的通知, 2025.03.14.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6'}\n","SPRi AI Brief2025년 4월호\n","4\n","인도 전자정보기술부, AI 강국 실현을 위한 AI 로드맵 공개n인도 전자정보기술부가 2047년까지 ‘선진 인도’ 비전과 AI 강국 실현을 위한 다양한 정책을 포괄하는 AI 로드맵을 발표n인도 정부는 2024년 3월 수립한 IndiaAI 미션을 중심으로 AI 컴퓨팅과 반도체 인프라 투자를 강화하는 한편, 데이터 접근성 확대와 AI 전문센터 설립, 자체 AI 모델 개발 등을 추진\n","KEY Contents\n","£인도 정부, IndiaAI 미션을 중심으로 AI 역량 구축을 위한 정책 추진n인도 전자정보기술부(MeitY)는 2025년 3월 6일 인도 독립 100년을 맞는 2047년까지 선진국으로 도약하겠다는 ‘선진 인도(Viksit Bharat)’ 비전 달성을 위한 다양한 AI 정책을 소개n(AI 컴퓨팅과 반도체 인프라) 2024년 3월 공개한 IndiaAI 미션*에서 5년간 약 1,030억 루피(한화 약 1조 7천억 원)를 AI 역량 강화에 할당* AI 컴퓨팅 성능 활성화, AI 혁신센터 설립, AI 통합 데이터 플랫폼 등 AI 역량 구축을 위한 국가 전략∙IndiaAI 미션을 통해 18,693개의 GPU를 갖춘 고성능 컴퓨팅 시설 구축을 추진하는 한편, 개방형 GPU 마켓플레이스를 구축해 스타트업과 연구자, 학생에게 컴퓨팅 자원을 지원n(데이터 공유 및 AI 전문센터 설립) 고품질 데이터셋에 대한 접근성을 확대하는 동시에, 학제 간 연구와 산학 협력을 통해 AI 솔루션 개발을 이끌 AI 전문센터(CoE, Center of Excellence)를 설립해 AI 발전을 지원∙IndiaAI 데이터셋 플랫폼을 구축해 스타트업과 연구자에게 고품질의 익명화 데이터셋을 제공하여 AI 모델의 정확성 개선을 지원하는 한편, 뉴델리에 △헬스케어 △농업 △지속 가능한 도시를 중점 연구하는 AI 전문센터 3곳을 설립한 데 이어 추가로 교육 분야에 특화된 AI 전문센터를 설립 예정n(인도의 AI 모델 개발) IndiaAI 미션을 통해 제안 요청을 받아 자체 AI 개발 이니셔티브를 시작하고, 2024년부터 정부 주도의 멀티모달 LLM 개발 프로젝트인 ‘BharatGen’을 진행n(디지털 공공 인프라와 AI 통합) 공공 플랫폼을 기반으로 민간 기업이 분야별 솔루션을 구축하는 방식으로 디지털 공공 인프라(DPI)와 AI의 통합을 확대∙일례로 2025년 1월 열린 인도의 대표적 힌두교 행사 마하쿰브에서 AI 기반 분석 도구로 철도 승객 흐름을 실시간 모니터링하여 군중 분산을 유도하고 AI 챗봇을 활용해 분실물 서비스, 실시간 번역, 다국어 서비스를 지원n(AI 인력 양성) 교육과 기술 개발에 대한 지속적 투자를 위해 AI 과목을 포함하도록 대학 커리큘럼을 개편∙‘IndiaAI Future Skills’ 이니셔티브를 통해 학부, 대학원, 박사 과정 전반으로 AI 교육을 확대하는 한편, 중소도시에 데이터 및 AI 연구소를 설립해 접근성을 확대n(실용적 규제) 법률에만 의존하기보다 주요 대학과 연구소에 대한 자금 지원을 통해 딥페이크, 개인정보 유출, 사이버 위협에 대응한 기술 솔루션을 개발하는 실용적인 규제 접근방식을 채택출처 | Ministry of Electronics & IT, India’s AI Revolution- A Roadmap to Viksit Bharat, 2025.03.06.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 6, 'page_label': '7'}\n","정책･법제기업･산업기술･연구인력･교육\n","5\n","영국 경쟁시장청, 마이크로소프트와 오픈AI 협력 관련 반독점 조사 종료 발표n영국 경쟁시장청이 14개월에 걸친 마이크로소프트와 오픈AI의 파트너십에 대한 반독점 조사를 종료하고 마이크로소프트가 오픈AI에 실질적 지배권을 행사하지 않았다는 결론을 제시n그러나 EU는 양사의 파트너십을 계속 주시하기로 했으며 미국 연방거래위원회(FCC)도 2024년 양사를 포함한 주요 AI 기업에 대한 조사에 착수하는 등, 규제 이슈는 지속 \n","KEY Contents\n","£영국 경쟁시장청, 마이크로소프트가 오픈AI에 실질적 지배권 행사하지 않았다고 평가 n영국 경쟁시장청(CMA)이 2025년 3월 5일 마이크로소프트(Microsoft)와 오픈AI(OpenAI)의 협력 관계에 대한 반독점 조사를 종료한다고 발표∙마이크로소프트는 오픈AI의 최대 투자자로서, 2019년 오픈AI와 파트너십을 맺고 2024년까지 130억 달러 이상을 투자했으며, 이에 CMA는 2023년 12월 양사의 파트너십이 영국 내 시장 경쟁을 저해할 수 있는 사실상의 합병으로 보아야 하는지에 대한 조사에 착수∙CMA는 14개월에 걸친 조사 결과, 마이크로소프트가 오픈AI에 대한 실질적인 영향력을 확보했으나 지배권 변동이 없으므로 인수합병 규정*에 따른 전면 조사의 대상이 되지 않는다는 결론을 제시* 영국 기업법(Enterprise Act 2002) 제22조(완결된 합병과 관련된 심층 조사 회부 의무)nCMA의 조엘 뱀포드(Joel Bamford) 합병 담당 이사는 2025년 3월 5일 비즈니스 소셜미디어 플랫폼 링크드인(Linked in) 게시글*을 통해 마이크로소프트가 오픈AI에 실질적인 지배권을 행사하지 않아 현재 형태의 파트너십이 영국의 합병 통제 제도에 따른 검토 대상이 되지 않는다고 설명* What are the key takeaways from our review of the Microsoft/OpenAI partnership?, Joel Bamford, 2025.03.05.∙그는 마이크로소프트가 2025년 1월 오픈AI에 대한 컴퓨팅 용량 공급 관련 계약상 권리를 독점 공급에서 우선협상권으로 변경하면서 오픈AI의 의존도가 감소한 점도 언급했으며, 파트너십의 복잡성과 계약 조건의 변화 및 계약의 실제 운영 방식 등을 검토하는 과정에서 조사에 이례적으로 오랜 기간이 걸렸다고 설명£EU와 미국의 규제당국은 여전히 마이크로소프트와 오픈AI의 파트너십 주시n마이크로소프트와 오픈AI의 파트너십에 대한 CMA의 승인에도, EU와 미국 등 주요국 규제당국은 여전히 양사의 관계에 주목∙EU 집행위원회는 2024년 양사의 파트너십에 대한 정식 조사는 불필요하다는 결론을 내렸으나 양사의 관계를 계속 주시하기로 했으며, 미국 연방거래위원회(FCC)는 2024년 양사를 포함한  주요 AI 기업들의 생성 AI 파트너십에 대한 조사에 착수∙이에 마이크로소프트는 2024년 오픈AI 이사회의 옵저버 역할을 포기하는 등, 오픈AI에 대한 영향력 축소를 시도출처 | Competition and Markets Authority, Microsoft / OpenAI partnership merger inquiry, 2025.03.05.The Wall Street Journal, Microsoft-OpenAI Partnership Gets U.K. Antitrust Clearance, 2025.03.05.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8'}\n","| 2025년 4월호 |\n","기업･산업\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 8, 'page_label': '9'}\n","정책･법제기업･산업기술･연구인력･교육\n","7\n","구글, 단일 GPU로 구동 가능한 오픈소스 AI 모델 ‘젬마 3’ 공개n구글이 휴대 단말에서 단일 GPU나 TPU로 구동할 수 있는 오픈소스 AI 모델 ‘젬마 3’를 매개변수 1B, 4B, 12B, 27B의 4개 버전으로 공개n구글에 따르면 젬마 3는 챗봇 아레나의 인간 선호도 평가에서 라마3-405B, 딥시크-V3, o3-미니를 능가하는 것으로 나타나 크기 대비 최고의 성능을 발휘 \n","KEY Contents\n","£젬마 3, 텍스트와 시각적 추론 및 최대 128K 토큰의 컨텍스트 창 지원n구글(Google)이 2024년 3월 12일 ‘제미나이(Gemini) 2.0’ 모델 아키텍처 기반의 경량 오픈소스 AI 모델 ‘젬마 3(Gemma 3)’를 공개∙스마트폰과 노트북 같은 단말에서 단일 GPU나 TPU*로 실행할 수 있는 젬마 3는 매개변수 1B, 4B, 12B, 27B의 4개 버전으로 제공되어 개발자들이 하드웨어나 성능 요구사항에 맞게 모델을 선택 가능* 구글에서 개발한 머신러닝 전용 프로세서로, 일반 GPU보다 머신러닝 작업에서 뛰어난 성능과 전력 효율성을 보유∙젬마 3는 140개 이상의 언어와 고급 텍스트 이해 및 시각 정보 추론을 지원하고, 최대 12만 8천 개 토큰의 컨텍스트 창 및 작업 자동화와 에이전트 구축을 위한 함수 호출 기능과 구조화된 출력을 제공하며, 양자화* 버전을 통해 모델 크기와 컴퓨팅 요구사항을 줄이면서 높은 정확도를 유지* 모델의 매개변수를 저장하는 데 필요한 비트 수를 줄여 모델 크기를 압축하고 연산 속도를 높이는 기술∙구글은 젬마 3가 챗봇 아레나(Chatbot Arena)* 인간 선호도 평가에서 1338점으로 매개변수가 훨씬 큰 모델인 라마3-405B(1269점), 딥시크-V3(1318점), o3-mini(1304점)를 능가했다는 점에서 크기 대비 최고의 성능을 발휘했다고 주장* 사용자 선호도를 통해 AI 모델을 평가하는 오픈소스 플랫폼\n"," <젬마 3와 주요 AI 모델의 챗봇 아레나 점수 비교>\n","n구글은 젬마 3에 대한 광범위한 데이터 거버넌스 및 미세조정, 벤치마크 평가를 통해 안전성을 높이는 한편, 젬마 3를 기반으로 이미지 안전성을 검사하는 솔루션 ‘쉴드젬마(ShieldGemma) 2’도 출시∙쉴드젬마 2는 △위험한 콘텐츠 △노골적인 성적 표현 △폭력 등 세 가지 범주에서 안전 라벨을 제공하여 유해 콘텐츠 필터링을 지원출처 | Google, Introducing Gemma 3: The most capable model you can run on a single GPU or TPU, 2025.03.12.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10'}\n","SPRi AI Brief2025년 4월호\n","8\n"," 오픈AI, AI 에이전트 구축 지원 도구와 신규 음성·이미지 생성 모델 출시n오픈AI가 AI 에이전트의 효율적 구축을 지원하는‘Response API’와 다중 에이전트 기반의 업무 프로세스 조율을 지원하는 ‘에이전트 SDK’를 출시n오픈AI는 개발자 대상으로 API에서 음성인식 및 음성합성 모델 3종을 공개하는 한편, GPT-4o를 기반으로 텍스트와 이미지 생성을 통합한 AI 모델도 공개\n","KEY Contents\n","£오픈AI의 Response API, 웹 검색과 파일 검색, 컴퓨터 사용 기능을 지원 n오픈AI가 2025년 3월 11일 개발자의 효율적인 AI 에이전트 구축을 지원하는 ‘Response API’를 공개∙Response API는 여러 API나 외부 공급업체를 사용하지 않고 오픈AI의 모델과 기본 제공 도구를 앱에 쉽게 결합하려는 개발자를 위한 API로, 웹 검색, 파일 탐색, 컴퓨터 사용(Computer Use)*등 기능을 지원* 브라우저 기반의 실제 작업(예: 파일 다운로드, 웹 검색 등)을 수행할 수 있는 오픈AI의 에이전트 기능∙오픈AI는 대화형 응답을 생성하는 ‘Chat Completions API’ 및 AI 어시스턴트 생성과 관리를 지원하는 ‘Assistants API’의 기능을 결합해 Response API를 출시했으며, 웹 검색과 파일 검색, 컴퓨터 사용 기능을 통해 AI 모델이 현실의 복잡한 작업을 수행할 수 있다고 설명n오픈AI는 Response API와 함께 개발자가 다중 에이전트 기반의 업무 프로세스 조율을 간소화할 수 있도록 지원하는 ‘에이전트 SDK*’도 공개* Software Development Kit: 특정 플랫폼이나 API를 쉽게 사용할 수 있도록 제공하는 도구 모음∙오픈AI는 2024년 10월 공개한 실험적 SDK ‘Swarm’을 바탕으로 LLM의 손쉬운 구성, 에이전트 간 지능적 연계 지원, 안전 가드레일 적용 등의 개선 사항을 적용해 에이전트 SDK를 출시£오픈AI, 신규 음성인식·음성합성과 이미지 생성 모델 출시 n오픈AI는 2025년 3월 20일 API로 음성인식(Speech-to-Text) 모델 ‘gpt-4o-transcribe’ 및 ‘gpt-4o-mini-trasncribe’와 음성합성(Text-to-Speech) 모델 ‘gpt-4o-mini-tts’를 출시∙이 모델들은 악센트, 소음이 있는 환경과 같은 까다로운 조건에서도 높은 정확도를 유지하는 것이 특징으로, 특히 음성인식 모델은 기존 ‘위스퍼(Whisper)’ 모델 대비 단어 오류율(WER)이 크게 개선 n오픈AI는 2025년 3월 25일 텍스트와 이미지 생성 기능을 통합해 더욱 정확하고 사실적인 이미지를 생성하는 ‘GPT-4o 이미지 생성(Image Generation)’ 모델도 출시∙오픈AI에 따르면 GPT-4o에서 이미지 생성 기능이 기본으로 제공되므로 GPT-4o의 지식 베이스와 대화 맥락을 활용해 사용자 요구를 세밀하게 반영한 이미지를 생성하며 이미지에 삽입되는 텍스트 품질도 크게 개선∙GPT-4o 이미지 생성 모델은 챗GPT의 기본 이미지 생성기로 유료 사용자뿐 아니라 무료 사용자에게도 제공되며, 개발자를 위한 API 접근도 몇 주 안에 지원될 예정출처 | OpenAI, New tools for building agents, 2025.03.11.OpenAI, Introducing 4o Image Generation, 2025.03.25.OpenAI, Introducing next-generation audio models in the API, 2025.03.20.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 10, 'page_label': '11'}\n","정책･법제기업･산업기술･연구인력･교육\n","9\n","앤스로픽, 트럼프 행정부에 국가 경제와 안보를 고려한 AI 정책 제안n앤스로픽은 경제와 국가안보에 막대한 영향력을 미칠 AI 기술에서 미국이 주도권을 유지하기 위해 트럼프 행정부가 중점적으로 추진해야 할 6가지 핵심 영역을 제안n앤스로픽은 AI 모델에 대한 국가안보 테스트의 강화와 첨단 반도체의 수출 통제 강화, AI 연구소 보안 강화, 에너지 인프라 확장, 정부의 AI 도입 촉진, AI의 경제적 영향 대비를 권고\n","KEY Contents\n"," £앤스로픽, AI 모델에 대한 국가안보 테스트와 첨단 반도체 수출 통제를 요구 n앤스로픽은 2025년 3월 6일 트럼프 행정부를 대상으로 AI가 국가 경제와 안보에 미치는 영향을 중점 고려한 AI 정책 권고안을 발표∙미국의 AI 리더십을 강화할 ‘AI 행동계획(AI Action Plan)’ 수립을 요구한 트럼프 대통령이 AI 행정명령 이행을 위해 백악관 과학기술정책실(OSTP)이 진행한 공개 의견 요청*에 대한 응답으로 정책을 제안* 미국의 AI 우위 유지와 강화에 필요한 정책 우선순위 수립 및 민간 혁신을 저해하는 규제 부담 해소를 위해 AI 행동계획에 우선 포함되어야 할 조치에 대하여 2025년 2월 6일부터 3월 15일까지 의견수렴을 진행  ∙앤스로픽은 2026년 후반이나 2027년 초에 ‘강력한 AI 시스템’*이 출현할 것으로 예상하며, AI가 경제와 국가안보에 미칠 막대한 영향력에 대비해 미국이 리더십을 유지하기 위한 단호한 정책 대응을 요구* 노벨상 수상자 수준의 지적 능력, 인간과 동등한 수준의 인터페이스 탐색 능력과 복잡한 작업의 자율적 추론 능력, 물리적 세계와 상호작용하는 능력을 갖춘 AI 시스템을 의미n앤스로픽은 AI가 경제와 국가안보에 미치는 영향을 고려하고 미국의 국익을 극대화하기 위해 중점을 두어야 할 6가지 핵심 정책 영역을 제시∙(국가안보 테스트 강화) 정부 기관은 국내외 AI 모델이 국가안보에 미치는 잠재적 영향을 평가할 수 있도록 표준 평가 프레임워크 수립, 안전한 테스트 인프라 구축, AI 시스템의 취약점을 분석할 수 있는 전문팀 구성과 같은 역량을 개발∙(첨단 반도체 수출 통제 강화) 미국과 동맹국이 강력한 AI 시스템의 이점을 누리는 동시에 중국 등의 적대국이 첨단 AI 인프라에 접근할 수 없도록 엔비디아 H20 칩*을 포함한 첨단 반도체 수출제한을 강화하며, 중국의 AI 칩 밀수를 방지하기 위한 정부 간 협정을 추진* 미국 수출 규제를 준수하며 중국 시장용으로 특별히 설계된 저사양 칩으로 H100 대비 연산 성능이 5분의 1 수준∙(AI 연구소 보안 강화) AI의 전략적 자산화를 고려해 AI 연구소와 정보기관 간 기밀 커뮤니케이션 채널을 구축하고, AI 전문가에 대한 보안 허가를 신속히 처리하며, AI 인프라용 차세대 보안 표준을 개발∙(에너지 인프라 확장) AI 개발을 지속적으로 선도하기 위해 2027년까지 50기가와트 규모의 AI 전용 전력을 추가 확보하고, 에너지 인프라 구축을 위한 허가와 승인 절차를 간소화∙(정부의 AI 도입 촉진) 정부 차원에서 AI 도입으로 혜택을 누릴 수 있는 업무 목록을 정리하고, 각 기관 책임자에게 AI가 공익에 크게 기여할 수 있는 프로그램 수립을 요구 ∙(AI의 경제적 영향 대비) AI의 도입 추세와 이에 따른 경제적 영향을 더욱 잘 파악할 수 있도록 인구조사국 조사와 같은 경제 데이터의 수집 체계를 개선하고, 이를 통해 AI로 인한 구조적 변화에 대비출처 | Anthropic, Anthropic’s Recommendations to OSTP for the U.S. AI Action Plan, 2025.03.06.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 11, 'page_label': '12'}\n","SPRi AI Brief2025년 4월호\n","10\n","오픈AI, 트럼프 행정부에 혁신의 자유 강조하는 AI 정책 제안n오픈AI는 트럼프 행정부를 대상으로 한 AI 정책 제안에서 혁신의 자유를 보장하기 위해 연방 정부와 민간 부문 간 자율적 협력 체제를 수립할 것을 권고n오픈AI는 또한 민주주의 원칙에 따른 AI 칩과 시스템 수출 통제 및 학습의 자유를 증진하는 저작권 체계 수립, 인프라 투자, 정부의 AI 도입 확대를 요구\n","KEY Contents\n","£오픈AI, 연방 정부와 민간 부문 간 자율적 협력 체제를 통한 혁신의 자유 요구n오픈AI가 트럼프 행정부의 ‘AI Action Plan’ 수립을 위한 백악관 과학기술정책실(OSTP)의 의견 요청에 대응해 2025년 3월 13일 혁신의 자유를 강조하는 정책 제안을 제출∙정책 제안은 혁신의 자유를 보장하는 규제 접근방식과 민주적 AI 원칙에 따른 수출 통제, 학습의 자유를 증진하는 저작권 체계, 인프라 기회 포착, 정부의 AI 도입 확대를 포괄n(혁신의 자유 보장) 국가안보를 강화하기 위해 연방 정부와 민간 부문 간 자율적 협력 체제를 수립하고, 주 정부의 파편화된 AI 규제로부터 AI 기업을 보호 ∙여러 주에서 발의한 AI 관련 법안으로 인한 규제 준수 부담이 중국에 반사이익이 되지 않도록, AI 기업이 연방 정부와 소통할 수 있는 단일 창구를 마련해 규제 불확실성을 제거하고 혁신의 자유를 보장n(민주주의 원칙에 따른 수출 통제) 글로벌 AI 시장에서 민주주의 원칙을 따르는 국가에는 미국산 AI 시스템의 도입을 장려하고, 중국 및 중국에 동조하는 소수 국가에는 수출을 통제∙중국 및 미국 무기 금수조치가 적용되는 국가들에는 첨단 칩에 대한 기존 수출 통제를 포함해 AI 시스템에 대한 엄격한 수출 통제 시행 강조n(학습의 자유를 증진하는 저작권 체계) 미국의 AI 리더십과 경제 및 국가안보를 지원하는 저작권 체계 수립을 위한 조치를 채택∙중국의 기술적 우위를 막기 위해 AI에 공정사용* 원칙을 적용하고, 미국 기업들의 데이터 접근권을 보장하기 위한 국제 정책 논의를 진행하며, 정부가 보유하거나 지원하는 데이터 접근성을 확대* Fair Use: 저작권자 허락 없이도 언론 보도나 교육 목적 등으로 콘텐츠 사용을 허용하는 원칙n(성장을 촉진하는 인프라 기회 포착) 중국과의 AI 주도권 경쟁에 필요한 인프라에 투자함으로써 경제 성장과 AI 접근성 극대화, 민감한 미국 데이터의 자국 내 저장을 실현해 국가안보 이익을 보호∙에너지 인프라에 대한 계획과 허가 절차를 간소화하여 인프라 건설을 촉진하고, 공립대학에 AI 기업의 컴퓨팅 자원을 제공해 AI 전문 인력의 양성을 지원n(정부의 AI 도입 확대) 전 세계적으로 민주적 AI가 발전하려면 미국 정부가 국민의 안전과 번영, 자유를 위해 AI를 사용하는 모범을 보여야 하며, 정부의 AI 도입을 강화하기 위한 공공-민간 파트너십을 통해 AI 도입 절차를 간소화하고 신속한 AI 조달 체계를 마련 출처 | OpenAI, OpenAI’s proposals for the U.S. AI Action Plan, 2025.03.13.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 12, 'page_label': '13'}\n","정책･법제기업･산업기술･연구인력･교육\n","11\n","엔비디아, GTC 2025에서 신규 AI 칩 로드맵과 AI 제품군 공개n엔비디아가 GTC 2025에서 AI 칩 로드맵을 공개하며, 2025년 하반기 블랙웰 울트라에 이어 2026년 하반기에 새로운 아키텍처의 ‘베라 루빈’을 출시하겠다고 발표n엔비디아는 피지컬 AI에 주목하며 휴머노이드 로봇용 개방형 기반모델 ‘아이작 그루트 N1’을 공개하는 한편, AI 에이전트 개발을 위한 오픈소스 모델 ‘라마 네모트론’도 함께 발표\n","KEY Contents\n","£엔비디아, 신규 AI 칩 로드맵 및 피지컬 AI와 AI 에이전트 관련 신제품 발표n엔비디아(Nvidia)가 2025년 3월 17~21일 미국 새너제이에서 열린 연례 개발자 회의 GTC 2025에서 향후 출시할 AI 칩 로드맵을 포함해 다양한 AI 신제품군을 소개∙젠슨 황(Jensen Huang) 엔비디아 CEO는 2025년 3월 18일 기조연설을 통해 AI 시장 전망을 제시하며 추론 모델과 AI 에이전트의 부상으로 AI 컴퓨팅 수요가 빠르게 증가하는 한편, AI 워크로드의 확장과 복잡성 증가로 인해 전 세계 데이터센터 투자가 급증하고 있다고 설명 n엔비디아는 2024년 출시한 GPU ‘블랙웰(Blackwell)’의 업그레이드 버전 ‘블랙웰 울트라’를 올해 하반기에 출시하고, 2026년 하반기에 새로운 아키텍처의 AI 칩 ‘베라 루빈(Vera Rubin)’을 출시 예정∙최대 288GB의 고대역폭 메모리를 탑재한 블랙웰 울트라는 블랙웰보다 1.5배 더 높은 AI 성능을 제공하며, AI 모델의 추론과 훈련을 더욱 정교하게 수행할 수 있도록 설계 ∙블랙웰의 뒤를 잇는 새로운 아키텍처의 베라 루빈은 ‘베라(Vera)’라는 맞춤형 설계 CPU를 탑재하게 되며, 2027년 하반기에는 성능과 메모리가 개선된 루빈 울트라를, 2028년에는 ‘파인만(Feynman)’을 출시 예정n엔비디아는 AI 기반 로보틱스와 자동화와 같은 피지컬 AI(Physical AI)가 장차 50조 달러 규모의 시장 기회를 형성할 것으로 예상하며, 휴머노이드 로봇용 AI 모델 ‘아이작 그루트 N1(Isaac GR00T N1)’을 발표 ∙아이작 그루트 N1은 휴머노이드 로봇을 위한 세계 최초의 개방형 기반모델로, 로봇이 다양한 작업을 수행할 수 있도록 사전훈련된 상태로 제공되며, 특정 목적에 맞게 추가 학습을 진행할 수 있도록 설계∙엔비디아는 구글 딥마인드, 디즈니 리서치와 함께 로봇이 복잡한 작업을 정밀하게 처리하는 방법을 학습할 수 있도록 지원하는 오픈소스 물리엔진 ‘뉴튼(Newton)’도 개발 중이라고 설명∙엔비디아는 AI를 가장 먼저 도입한 분야로 자율주행차량을 꼽으면서 차세대 자동차와 공장, 로봇 개발에서 GM과의 협력을 소개하는 한편, 자율주행차량 종합 안전 시스템 ‘헤일로스(Halos)’도 발표n엔비디아는 이번 행사에서 개발자와 기업을 대상으로 추론 기능을 갖춘 오픈소스 AI 모델 ‘라마 네모트론(Llama Nemotron)’ 제품군도 공개∙메타의 라마 모델을 기반으로 구축된 라마 네모트론은 복잡한 작업을 해결할 수 있는 AI 에이전트 구축을 위한 모델로, 엔비디아는 사후 훈련을 통해 다단계 수학, 코딩, 추론 및 복잡한 의사 결정 능력을 개선출처 | Nvidia, GTC 2025 – Announcements and Live Updates, 2025.03.20.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 13, 'page_label': '14'}\n","SPRi AI Brief2025년 4월호\n","12\n","마누스 AI, 완전 자율 AI 에이전트 ‘마누스’ 공개n중국의 AI 스타트업 마누스 AI가 다양한 작업을 인간의 개입 없이 독립적으로 실행할 수 있는 완전 자율 AI 에이전트 ‘마누스’를 발표하고 초대 시스템을 통해 일부 사용자에게 제공n마누스는 앤스로픽 클로드와 알리바바 큐원 등 기존 LLM을 활용한 다중 에이전트 시스템으로 설계되었으며, 최초의 완전 자율 AI 에이전트라는 점에서 제2의 딥시크라는 평가도 존재\n","KEY Contents\n","£마누스 AI, 인간의 개입 없이 독립적으로 작업 수행하는 AI 에이전트 발표 n중국 AI 스타트업 마누스 AI(Manus AI)가 2025년 3월 5일 유튜브와 전용 웹사이트를 통해 완전 자율 AI 에이전트 ‘마누스’를 공개∙마누스 AI는 전용 웹사이트(Manus.im)에서 마누스가 “사고와 행동을 연결하는 세계 최초의 범용 AI 에이전트로, 업무와 일상의 다양한 작업에서 뛰어난 능력을 발휘”한다고 주장∙웹사이트에 소개된 활용 사례에 따르면 마누스는 여행계획 수립, 부동산 조사, 보험 정책 비교 등 다양한 작업을 인간의 개입 없이 독립적으로 수행하며, 일례로 이력서 파일을 받으면 후보자 순위 결정을 넘어 후보자들의 보유 기술을 추출하고 일자리 동향과 교차 참조하여 최적화된 채용 결정을 제시∙마누스 AI는 마누스가 GAIA(General AI Assistants)* 벤치마크 테스트에서 오픈AI의 딥 리서치를 능가하는 최고 점수를 달성했다고 보고*** AI 모델의 추론, 멀티모달 처리, 웹 브라우징, 도구 사용 등 복잡한 능력을 평가하는 벤치마크로 3가지 난이도로 구성** (GAIA 레벨별 성능 비교) Level 1: Manus.ai 86.5%, OpenAI Deep Research 74.3%/Level 2: Manus.ai 70.1%, OpenAI Deep Research 69.1%/Level 3 : Manus.ai 57.7%, OpenAI Deep Research  47.6% ∙마누스는 초대 코드를 통해서만 접근을 허용하는 비공개 테스트 단계로, 마누스 AI는 서버 용량 제한으로 인해 초대 시스템이 불가피하며 이를 해결하기 위해 노력 중이라고 설명n미국 경제매체 포브스(Forbes)는 마누스가 “인간의 조수에 그치지 않고 인간을 대체하는 세계 최초의 완전 자율 AI 에이전트”라며, 제2의 딥시크라고 평가했으나 부정적인 평가도 존재∙포브스에 따르면 마누스는 앤스로픽 클로드(Claude)와 알리바바 큐원(Qwen)과 같은 기존 LLM 기반 다중 에이전트 시스템으로 설계되어 복잡한 작업을 구성 요소별로 나누어 적절한 에이전트에 할당하고 진행 상황을 감독하며, 사용자의 개입 없이 백그라운드에서 작업을 수행한 뒤 완료되면 사용자에게 안내∙일부 초기 테스터 사이에서 정보 검색 실수와 시스템 다운을 비롯한 마누스의 오류나 단점에 대한 지적도 나오고 있으나, 포브스는 정식 출시에 앞서 시스템이 개선될 것으로 예상∙그러나 미국의 IT 미디어 테크크런치(TechCrunch)는 마누스가 사용자에게 잘못된 답변을 제시하거나 주문이나 예약 프로세스에 종종 실패하는 등 잦은 오류를 보였으며, 딥시크와 달리 자체적으로 모델을 개발하지도 않았다며 제2의 딥시크라는 표현은 과장되었다고 지적출처 | Manus AI, Introducing Manus: The General AI Agent, 2025.03.05. Forbes, China’s Autonomous Agent, Manus, Changes Everything, 2025.03.08.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 13, 'page_label': '14'}\n","TechCrunch, Manus probably isn’t China’s second ‘DeepSeek moment’, 2025.03.09.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15'}\n","정책･법제기업･산업기술･연구인력･교육\n","13\n","피규어, 휴머노이드 로봇 제조시설 ‘BotQ’ 발표 n피규어가 자체 생산이 가능한 휴머노이드 로봇 제조시설 BotQ를 공개하고 올해 안에 자체 휴머노이드 로봇을 BotQ의 제조 공정에 투입해 자동화 수준을 높일 계획n피규어는 휴머노이드 로봇 양산을 위해 로봇 아키텍처 재편, 공급망 구축, 제조팀 구성, 소프트웨어 인프라 구축, 로봇 자동화 등을 통해 대량생산이 가능한 확장형 제조시설을 구축했다고 강조\n","KEY Contents\n","£피규어, 연간 1만 2천 대의 휴머노이드 로봇을 생산할 수 있는 제조시설 구축n미국 휴머노이드 로봇 스타트업 피규어(Figure)가 2025년 3월 15일 연간 최대 1만 2천 대의 로봇을 생산할 수 있는 휴머노이드 로봇 제조시설 ‘BotQ’를 공개∙피규어는 휴머노이드 로봇을 자체 생산함으로써 제작 공정과 품질을 통제하여 고성능 로봇을 시장에 출시할 수 있을 것으로 기대 ∙올해 안에 자체 휴머노이드 로봇을 BotQ의 제조 공정에 투입할 계획으로, 휴머노이드 로봇 투입을 점진적으로 늘려 생산 설비의 자동화 수준을 향상할 예정 n피규어는 휴머노이드 로봇의 아키텍처 재편, 공급망 구축, 전문 제조팀 구성, 소프트웨어 인프라 구축, 로봇 자동화 등을 통해 확장성 있는 제조시설을 건설∙피규어가 2024년 8월 공개한 2세대 휴머노이드 로봇 ‘피규어 02’는 고도의 정밀성을 요구하여 복잡하고 속도가 느린 컴퓨터 수치 제어(CNC)* 공정을 주로 사용했으나, 차세대 ‘피규어 03’ 설계 시에는 사출 성형 방식**으로 전환해 CNC 기계로 일주일 이상 소요되던 부품 생산을 강철 금형으로 20초 이내에 완료* Computer Numerical Control: 컴퓨터를 사용하여 공작 기계를 자동 제어하는 기술 ** 고분자 재료를 녹여 금형에 주입한 후 냉각시켜 제품을 대량으로 생산하는 제조 공정∙피규어는 로봇 아키텍처를 재편하면서 안전팀과 신뢰성 팀을 새로 구성했으며, 신뢰성 팀은 로봇 수명을 빠르게 평가할 수 있는 가속 수명주기 테스트를 통해 로봇의 신뢰성 지표 달성을 위한 개선 사항을 도출 ∙공급망 구축에서는 핵심 기술(액추에이터, 로봇 핸드, 배터리, 최종 조립)은 내부에서 확보하고 필요시 외부 공급업체를 활용하는 방식으로 향후 4년 내 휴머노이드 로봇 10만 개, 액추에이터 300만 개까지 생산 규모 확장이 가능해질 전망∙제조설비 설계와 최적화에 전문성을 갖춘 제조팀을 구성하고 품질과 속도 등의 지표를 반영해 단기적으로 자동화할 핵심 공정을 선택함으로써 생산 주기를 단축∙소프트웨어 인프라에서는 제품 수명주기 관리(PLM) 시스템, 전사적 자원관리(ERP) 시스템, 창고관리 시스템(WMS)과 함께 맞춤형 제조 실행 시스템(MES)*을 도입해 공급망을 통한 부품 추적에서 품질관리까지 전체 생산 절차를 효율적으로 운영* 제조 현장에서 생산 계획, 작업 지시, 품질관리 등 생산 활동을 실시간으로 관리하고 제어하는 시스템∙피규어는 자사 제조공정에 휴머노이드 로봇 자동화를 통합하고, AI 기반 자동화 시스템과 인간의 감독을 결합해 로봇으로 로봇을 제작하여 생산 일정을 가속화하고 미래의 자율 제조 실현을 위한 기반을 마련출처 | Figure, BotQ: A High-Volume Manufacturing Facility for Humanoid Robots, 2025.03.15.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 15, 'page_label': '16'}\n","SPRi AI Brief2025년 4월호\n","14\n","앱트로닉, 자빌과 전략적 제휴로 휴머노이드 로봇 제작 추진n앱트로닉이 자빌과 전략적 제휴를 체결하고 자빌의 제조 공정에 휴머노이드 로봇 ‘아폴로’를 시범 배치해 검증 테스트를 진행한다고 발표n앱트로닉은 휴머노이드 로봇 핵심 부품의 단가를 절감하고 자빌을 통해 양산 능력을 확보함으로써 제작 비용을 낮춰 산업용을 시작으로 의료, 가정 등으로 응용 분야를 확대할 계획 \n","KEY Contents\n","£자빌, 앱트로닉의 휴머노이드 로봇 제작 및 자사 제조 공정에 시범 도입 예정n미국 휴머노이드 로봇 기업 앱트로닉(Apptronik)이 2025년 2월 25일 글로벌 엔지니어링 및 제조, 공급망 솔루션 기업 자빌(Jabil)과 전략적 제휴를 발표∙자빌은 앱트로닉의 휴머노이드 로봇 ‘아폴로(Apollo)’를 제작하고 이를 자사 제조 공정에 활용하는 시범 프로젝트를 진행할 계획∙앱트로닉은 아폴로를 고객 현장에 배치하기에 앞서 자빌의 제조 공정에서 검사, 분류, 라인 내 물류 배송, 부품 조립과 같은 단순하고 반복적인 작업을 통한 검증 테스트를 시행할 예정∙자빌과 앱트로닉은 실제 생산 환경에 아폴로를 도입함으로써 기존 인력의 작업을 보완하는 한편, 제조 자동화와 아폴로의 AI 모델 최적화를 위한 실제 사용 사례를 수집할 수 있을 것으로 기대n앱트로닉은 휴머노이드 로봇의 대량 보급을 위해 핵심 부품인 액추에이터의 단가를 절감한 데 이어 자빌을 통해 확보한 양산 능력을 바탕으로 제작 비용을 낮춰 소매 업장이나 노인 돌봄, 궁극적으로는 가정으로 시장을 확장할 계획∙앱트로닉은 삶의 모든 측면에서 인간을 지원하는 휴머노이드 로봇 제작을 기업 목표로 제시했으며, 우선 제조와 물류 같은 중요 산업에서 시작해 의료, 가정까지 로봇 응용 분야를 확대할 예정\n"," <앱트로닉의 휴머노이드 로봇 ‘아폴로’>\n","출처 | Jabil, Apptronik and Jabil Collaborate to Scale Production of Apollo Humanoid Robots and Deploy in Manufacturing  Operations, 2025.02.25.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 16, 'page_label': '17'}\n","| 2025년 4월호 |\n","기술･연구\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18'}\n","SPRi AI Brief2025년 4월호\n","16\n","구글 딥마인드, 로봇 특화 AI 모델 ‘제미나이 로보틱스’ 개발n구글 딥마인드는 제미나이 2.0 기반의 시각-언어-행동 모델로 로봇 직접 제어를 위한 물리적 동작이 출력으로 추가된 로봇 특화 AI 모델 ‘제미나이 로보틱스’를 공개n구글 딥마인드는 물리적 세계에 대한 이해 능력을 개선해 로봇 제어에 필요한 모든 추론 단계를 즉각 수행할 수 있는 비전-언어 모델 ‘제미나이 로보틱스-ER’도 공개\n","KEY Contents\n","£‘제미나이 로보틱스’, 학습 과정에서 접하지 못한 작업도 즉각 수행하는 범용성 보유n구글 딥마인드(Google Deepmind)가 2025년 3월 12일 로봇 개발에 특화된 AI 모델 ‘제미나이 로보틱스(Gemini Robotics)’를 공개∙제미나이 2.0을 기반으로 구축된 제미나이 로보틱스는 첨단 시각-언어-행동(VLA) 모델로, 로봇을 직접 제어하기 위해 물리적 행동을 새로운 출력 형태로 추가했으며, 학습 과정에서 접하지 못한 새로운 물체나 환경에서도 기존에 학습한 지식을 응용해 다양한 작업을 즉각 수행하는 범용성을 보유* Vision-Language-Action: 시각, 언어, 행동을 통합적으로 이해하고 처리하는 AI 모델∙구글 딥마인드는 종합적인 범용성 벤치마크에서 제미나이 로보틱스가 다른 첨단 시각-언어-행동 모델보다 평균 두 배 이상의 성능 향상을 보였다고 보고*n구글 딥마인드에 따르면 제미나이 로보틱스는 제미나이의 언어 이해 능력을 바탕으로 일상적 대화체의 명령을 이해하고 다양한 언어로 답변할 수 있으며, 복잡한 다단계 작업도 수행 가능∙주변 환경을 지속적으로 모니터링하여 환경이나 명령의 변화를 감지하고 즉시 행동을 수정할 수 있어, 다양한 환경에서 로봇과 협력할 수 있도록 지원∙종이접기처럼 섬세한 운동 능력과 정밀한 조작을 요구하는 작업을 수행할 수 있으며, 로봇 유형별로 쉽게 적응할 수 있도록 설계되어 많은 연구실에서 사용되는 양팔 로봇 플랫폼뿐 아니라 휴머노이드 로봇과 같은 복잡한 형태에도 특화 가능£로봇 제어에 필요한 추론 수행 능력을 강화한 ‘제미나이 로보틱스-ER’도 공개n구글 딥마인드는 제미나이 로보틱스와 함께 첨단 시각-언어 모델 ‘제미나이 로보틱스-ER (Embodied Reasoning)’도 공개∙제미나이 로보틱스-ER은 물리적 세계에 대한 이해 능력을 향상한 모델로서, 3D 인식이나 상태 추정, 공간 이해, 계획과 코드 생성 등 로봇 제어에 필요한 모든 추론 단계를 즉각 수행 가능∙향상된 공간 추론 능력과 제미나이의 코딩 능력을 결합해 즉각적으로 새로운 기능을 구현할 수 있으며, 일례로 커피잔을 보여주면 잔을 들어올리기에 적합한 그립 방식과 안전한 접근 경로를 직관적으로 파악∙모델 안전성을 위해 충돌 회피와 같은 기본적인 안전 제어장치와 상호작용할 수 있으며, 제미나이의 안전 기능을 바탕으로 주어진 시나리오 안에서 안전한 동작을 판단하고, 질문에 적절한 답변을 생성하도록 설계 출처 | Google Deepmind, Gemini Robotics brings AI into the physical world, 2025.03.12.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 18, 'page_label': '19'}\n","정책･법제기업･산업기술･연구인력･교육\n","17\n","사카나 AI의 AI 생성 논문, ICLR 2025 워크숍에서 동료 심사 통과n사카나 AI는 자사의 ‘AI 사이언티스트’가 인간의 개입 없이 전적으로 생성한 논문이 ICLR 2025 워크숍에서 동료 심사를 통과했다고 발표n사카나 AI는 해당 논문이 정규 학회 수준에는 미치지 못하며 내부 검토에서도 일부 오류가 발견되었다고 한계를 밝히는 한편, AI 생성 과학 연구에 대한 규범의 필요성을 강조 \n","KEY Contents\n","£AI 사이언티스트로 생성된 논문 3편 중 1편이 ICLR 2025 워크숍 동료 심사 통과n일본 AI 스타트업 사카나 AI(Sakana AI)가 2025년 3월 12일 자사의 ‘AI 사이언티스트(AI Scientist)’로 생성된 논문이 국제 AI 학술대회 워크숍의 동료 심사(Peer-Review)를 통과했다고 발표∙사카나 AI는 2024년 8월 과학 연구를 자동화하는 ‘AI 사이언티스트’를 공개했으며, 이를 개선한 v2 버전으로 생성된 논문을 AI 분야의 대표적 학술대회인 ICLR 2025 워크숍에 제출∙AI 사이언티스트-v2는 과학적 가설 제안, 가설을 검증할 실험 제안, 실험 수행을 위한 코드 작성과 개선, 실험 수행, 데이터 분석과 시각화를 포함해 전체 과학 논문을 인간의 도움 없이 처음부터 끝까지 작성∙사카나 AI는 연구 수행을 위한 대략적인 주제만 제공하여 생성된 총 3편의 AI 생성 논문을 사전 협의 후 ICLR 워크숍에 제출했으며, 이 중 1편이 동료 심사를 통과했다고 설명∙동료 심사에서 통과된 논문은 신경망 훈련 기법에 관한 내용을 다루고 있으며, 심사 평점은 6.33점으로 제출된 전체 논문의 상위 45% 수준을 기록 £사카나 AI의 내부 검토 결과, AI 생성 논문의 품질은 정규 학회 수준에는 미달n사카나 AI는 투명성과 윤리를 고려해 동료 심사 통과 후 논문을 철회했기에, ICLR 주최 측이 논문에 대한 메타 심사*를 진행하지 않았으나 이론적으로 메타 심사 단계에서 논문이 거부되었을 가능성이 있다고 부연* 개별 심사자들의 의견을 종합하여 최종 평가를 제공하는 심사 과정∙또한 ICLR 학회의 논문 수락률이 통상 20~30% 수준이나 학회에서 개최하는 워크숍의 수락률은 60~70%에 달한다며, 내부 검토 결과 AI 생성 논문 3편 모두 정규 학회 수준에는 도달하지 못했다고 한계를 인정∙사카나 AI의 내부 검토에서는 AI 생성 논문에서 그림이나 인용 누락, 서식 문제 등의 오류가 확인되었으며, 향후 최고 수준의 학회 기준을 통과할 수 있는 고품질의 논문 생성을 위해 프로세스를 개선할 계획n사카나 AI는 이번 실험을 계기로 AI가 생성한 과학 연구의 투명성과 윤리성 측면에서 과학 커뮤니티 차원의 규범 개발이 필요하다고 강조∙해당 규범에는 논문이 전적으로나 부분적으로 AI로 생성되었다고 알리는 시점과 방법 등이 포함되어야 하며, 가능한 많은 정보를 공개하여 연구의 투명성 확보가 중요∙사카나 AI는 AI 사이언티스트의 개발 현황에 관하여 연구 커뮤니티와 의견을 지속적으로 교환하여 향후 AI가 동료 심사 통과만을 추구해 학문적 검토 과정의 의미를 훼손하지 않도록 유의할 것이라고 강조출처 | Sakana AI, The AI Scientist Generates its First Peer-Reviewed Scientific Publication, 2025.03.12.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20'}\n","SPRi AI Brief2025년 4월호\n","18\n","AAAI, 주요 AI 연구 주제와 해결 과제를 정리한 보고서 발간nAAAI가 AI의 빠른 발전 속에 역동적 변화를 겪고 있는 AI 연구의 흐름을 체계적으로 포착하기 위해 주요 AI 연구 주제를 분석한 보고서를 발간n보고서는 △AI 추론 △AI의 사실성과 신뢰성 △AI 에이전트 △AI 평가 △AI 윤리와 안전 △체화 AI를 비롯한 핵심 AI 연구 주제에 대하여 역사와 현재 추세, 해결 과제를 제시 \n","KEY Contents\n","£AAAI, AI 추론과 AI의 사실성과 신뢰성, AI 에이전트 등 핵심 AI 연구 주제 분석n세계적 권위의 AI 학회 AAAI(The Association for the Advancement of Artificial Intelligence)가 2025년 3월 2일 AI 연구와 관련된 주요 주제의 역사와 추세, 해결 과제를 정리한 보고서를 발간∙AI 기술의 빠른 발전으로 AI 연구 역시 주제와 방법, 연구 커뮤니티, 작업 환경을 포함한 여러 측면에서 역동적 변화를 겪고 있으며, AAAI는 이러한 변화 속에서 AI 연구의 흐름을 체계적인 방식으로 포착하기 위한 목적으로 이번 보고서를 작성n보고서는 △AI 추론 △AI의 사실성과 신뢰성 △AI 에이전트 △AI 평가 △AI 윤리와 안전 △체화 AI와 같은 최근 AI의 핵심 연구 주제에 대하여 다음과 같이 분석∙(AI 추론) 대규모 사전훈련을 거친 LLM은 자동화된 추론 기능에서 상당한 발전을 이루었으나, 추론의 정확성과 깊이를 보장하기 위한 더 많은 연구가 필요∙(AI의 사실성과 신뢰성) AI 시스템의 신뢰성(Trustworthiness) 부족은 중요 애플리케이션에서 AI 시스템 도입을 가로막는 핵심 요인으로, 사실성과 신뢰성 개선을 위해 미세조정, 검색증강 생성(RAG)*과 같은 기술이 사용되고 있으나 근미래에 이러한 문제가 충분히 해결되기는 어려울 전망* LLM의 응답을 생성하기 전 외부의 신뢰할 수 있는 지식 베이스를 참조하는 기술∙(AI 에이전트) AI 구성 요소를 모듈형 시스템으로 구조화해 투명성과 적응성을 개선하는 다중 에이전트 아키텍처 및 AI 에이전트와 인간 간 협력과 공동 의사결정을 강조하는 ‘협력형 AI’가 부상하는 가운데, 다중 에이전트 환경의 확장성과 투명성, 컴퓨팅 효율성을 개선하기 위한 추가 연구가 필요 ∙(AI 평가) 현재의 AI 평가 방식은 AI 모델의 성능을 평가하는 벤치마크 위주의 테스트에 중점을 두고 사용성과 투명성, 윤리 지침 준수와 같은 다른 중요 요소의 평가를 간과하고 있으며, 신뢰성 있는 AI 시스템의 대규모 배포를 위해 AI 시스템 평가를 위한 새로운 통찰력과 방법이 필요∙(AI 윤리와 안전) AI 기반 사이버 범죄와 자율 무기 등의 새로운 위협과 AI 윤리 문제를 해결할 기술·규제적 장치가 부족한 상태로, 학제 간 협업과 지속적 감독, AI 개발에 대한 명확한 책임 할당이 중요∙(체화 AI) 물리적 실체와 실제 환경을 결합하는 체화 AI(Embodied AI)는 주로 로봇에 대하여 시뮬레이션과 현실 환경에서 수많은 시행착오를 통한 강화학습 방식으로 구현되며, 인간 수준의 성능을 지닌 지능형 체화 에이전트를 구축할 수 있는 역량은 아직 확보되지 않았으나 근본적 장애물은 없다는 평가출처 | AAAI, AAAI 2025 Presidential Panel on the Future of AI Research, 2025.03.02.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21'}\n","정책･법제기업･산업기술･연구인력･교육\n","19\n","2024년 튜링상, 강화학습 연구에 기여한 연구자 2인이 수상nACM이 2024년 튜링상 수상자로 1980년대 부터 강화학습의 주요 개념을 제시하고 수학적 기초를 확립하며 중요 알고리즘을 개발한 바르토 교수와 서튼 교수를 선정nACM은 바르토와 서튼이 수십년 전 개발한 강화학습 알고리즘이 최근 15년간 심층학습 알고리즘과 결합되며 실제 응용 분야에서 큰 진전을 이루었다고 평가 \n","KEY Contents\n","£ACM, 강화학습의 주요 알고리즘을 개발한 바르토와 서튼 교수에 튜링상 수여n세계 최대의 컴퓨터학회 ACM(Association for Computing Machinery)은 2025년 3월 5일 2024년 튜링상 수상자로 강화학습 분야의 연구에 이바지한 연구자 2인을 선정∙컴퓨터과학의 아버지라 불리는 영국 수학자 앨런 튜링(Allen Turing)의 이름을 딴 튜링상은 컴퓨터과학의 노벨상으로 불리며 구글의 후원으로 100만 달러의 상금을 수여∙ACM은 2024년 튜링상 수상자로 미국 매사추세츠⼤ 애머스트 캠퍼스의 앤드루 바르토(Andrew Barto) 정보·컴퓨터과학과 명예 교수와 캐나다 앨버타⼤의 리처드 서튼(Richard Sutton) 컴퓨터과학과 교수를 선정nACM에 따르면 바르토와 서튼 교수는 1980년대 이래 일련의 논문을 통해 강화학습의 주요 아이디어를 소개하고 수학적 기초를 구축했으며 강화학습의 중요 알고리즘을 개발∙1950년대 앨런 튜링이 보상과 처벌에 기반한 기계학습 접근방식을 제안한 이래 수십 년간 진전이 없던 강화학습 연구에서 바르토와 서튼은 1980년 초 심리학적 관찰을 토대로 강화학습을 문제 해결을 위한 일반적인 프레임워크로 공식화∙두 연구자는 1998년 ‘Reinforcement Learning: An Introduction*’을 공동 저술했으며, 7만 5천 회 이상 인용된 이 교과서는 현재까지도 강화학습 분야의 표준 참고문헌이라는 평가*https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdfnACM은 수십 년 전 개발된 바르토와 서튼의 강화학습 알고리즘이 지난 15년간 심층학습 알고리즘과 통합되며 실제 적용에서 주요 진전이 이루어졌다고 설명∙강화학습의 대표적인 사례는 2016년과 2017년 당대 최고의 바둑 기사를 상대로 승리한 알파고(AlphaGo)가 있으며, 오픈AI의 챗GPT 역시 인간의 피드백을 이용한 강화학습(RLHF) 기술을 적용∙강화학습은 로봇 손 조작과 네트워크 혼잡 제어, 칩 설계, 글로벌 공급망 최적화, 챗봇의 행동 및 추론 개선을 비롯한 다양한 분야에서도 성공적으로 도입n한편, 바르토와 서튼은 현재의 AI 개발 속도를 경계하며 AI 기업들이 제품을 철저히 테스트하지 않고 서둘러 출시하는 경향에 우려를 표시∙바르토 교수는 현재 일부 AI 기업들이 연구를 발전시키는 대신 사업적 이윤의 관점에서 막대한 자금을 투자하고 있다며, 이러한 경향을 비판하고 안전한 엔지니어링 관행의 중요성을 강조출처 | ACM, ACM A.M. Turing Award Honors Two Researchers Who Led the Development of Cornerstone AI Technology, 2025.03.05.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21'}\n","Financial Times, Turing Award winners warn over unsafe deployment of AI models, 2025.03.05.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 21, 'page_label': '22'}\n","SPRi AI Brief2025년 4월호\n","20\n","카카오, 자체 개발 AI 모델 ‘카나나’의 테크니컬 리포트 공개n카카오가 자체 개발 LLM ‘카나나’의 테크니컬 리포트를 공개하고, 카나나에 단계별 사전학습과 가지치기, 지식증류 등의 학습 기법을 적용해 학습 효율성을 극대화했다고 설명  n카카오는 연구 목적의 활용을 지원하기 위해 온디바이스 환경에서 작동될 수 있는 경량 모델 ‘카나나 나노 2.1B’를 오픈소스로 공개\n","KEY Contents\n","£카나나, 단계별 사전학습과 지식 증류 등의 학습 기법으로 학습 효율성 극대화n카카오가 2025년 2월 27일 자체 개발 언어모델 ‘카나나(Kanana)’의 구조와 학습 과정, 성능을 상세히 다룬 테크니컬 리포트*를 공개* Kanana: Compute-efficient Bilingual Language Models(https://arxiv.org/html/2502.18934v3/)∙카카오는 2024년 10월 개발자 컨퍼런스 ‘if(kakaoAI)2024’에서 카나나 언어모델 라인업(카나나 플래그, 에센스, 나노)을 처음 공개했으며, 2024년 말 카나나 플래그의 개발을 완료해 전체 라인업을 구축∙카카오에 따르면 카나나 플래그는 글로벌 최고 수준의 한국어·영어 성능을 보였으며, 특히 한국어 지식을 평가하는 벤치마크 KMMLU에서는 경쟁 모델을 압도하는 한국어 처리 성능을 달성** Kanana Flag 32.5B: 64.19, Qwen2.5B 32B: 59.37, Gemma2 27B: 49.98, EXAONE-3.5-32B: 55.44n카카오는 카나나의 학습 효율을 극대화하기 위해 단계별 사전학습(Staged Pre-training), 가지치기(Pruning)*, 지식 증류(Distillation)**와 같은 다양한 학습 기법을 적용했다고 설명*모델 구성 요소를 가지치기해 중요 요소만 남기는 기법  **큰 모델의 지식을 더 작은 모델로 전달하는 기법∙먼저 단계별 사전학습을 통해 8B와 26.8B 크기의 모델을 각각 학습하고, 8B 모델을 바탕으로 모델 최적화를 위한 가지치기와 증류 과정을 거쳐 경량 모델 카나나 나노 2.1B를 구축∙또한 8B와 26.8B 모델에 ‘깊이 업스케일링(Depth UP-Scaling)*’ 기법을 적용하여 카나나 에센스 9.8B와 카나나 플래그 32.5B를 개발*기존 모델의 레이어를 더 많이 쌓아 올려 모델 규모를 효과적으로 키우는 방식∙이처럼 이미 학습된 모델을 활용해 다양한 크기의 모델을 개발함으로써 유사 크기의 글로벌 모델 대비 절반 이하의 학습 비용으로 학습 자원을 최적화했다고 강조(Qwen 2.5 대비 18% 수준) £카카오, 연구 목적의 활용을 위해 카나나 나노 2.1B 오픈소스 공개n한편, 카카오는 카나나 언어모델 중 연구 목적으로 활용할 수 있도록 ‘카나나 나노 2.1B’의 base 및 instruct 버전과 문서 임베딩용* 버전을 깃허브(Github)에 오픈소스로 공개 * 텍스트 문서의 내용을 컴퓨터가 이해할 수 있는 숫자 형태로 변환하는 기술∙카카오에 따르면 카카오 나노 2.1B는 온디바이스 환경에서 원활하게 구동되도록 설계되었으며 비교적 작은 모델임에도 유사 크기의 글로벌 모델과 견줄 수 있는 성능을 발휘하나, 경량 모델 특성상 복잡한 추론이나 수학 문제 해결 등 고난도 태스크에서는 한계를 내포출처 | 카카오, 카카오의 언어모델, Kanana 테크니컬 리포트 공개, 2025.02.27.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 22, 'page_label': '23'}\n","| 2025년 4월호 |\n","인력･교육\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24'}\n","SPRi AI Brief2025년 4월호\n","22\n","영국 옥스퍼드⼤ 연구 결과, AI 인력 채용 시 학위보다 실무기술이 중요n영국 옥스퍼드인터넷연구소가 2018~2024년 영국 내 온라인 구인 공고를 분석한 결과, 시간이 흐를수록 AI 일자리에서 학위보다 실무기술을 요구하는 경향이 증가n연구진은 영국 노동시장에서 AI 전문가 채용과 임금 책정 시 학위보다 실무기술과 산업별 노하우가 중요해지고 있다며 정규교육 이외의 다양한 대안을 확대할 것을 권고\n","KEY Contents\n","£영국 기업들, 2018~2024년 사이 AI 일자리보다 실무기술을 요구하는 경향 확대n옥스퍼드⼤ 산하 옥스퍼드인터넷연구소(OII)가 2025년 2월 26일 발표한 연구 결과에 따르면 AI 분야의 기업들은 AI 인력 채용 시 정규학위보다 실무기술을 중시∙기존 학술 연구*에 따르면 기업 수요가 노동 공급을 앞서면서 노동력 부족을 겪는 산업에서는 고용주들이 구직자의 학위보다 실제 기술에 주목하는 경향이 확인* Skills-based hiring is on the rise, J Fuller, Harvard Business Review, 2022∙연구진은 AI 중심의 기술 혁명을 오늘날 노동시장을 좌우하는 핵심적인 장기적 추세로 인식하고, AI 분야에서도 고용주들이 형식적인 학위보다 실제 기술 기반의 채용 관행을 적용하는지를 조사n연구진이 2018년 1월부터 2024년 6월 사이 영국 내 약 1,100만 개의 온라인 구인 공고를 분석한 결과, 시간의 흐름에 따라 AI 일자리에서 학위보다 실무기술을 요구하는 경향이 증가∙2018~2023년 사이 하나 이상의 AI 기술을 요구하는 구인 공고는 21% 증가했으며, 특히 2023년 이후 AI 기술 수요가 빠르게 증가하면서 영국 기업들이 사업 전반에 걸쳐 AI 기술을 수용하고 있음을 시사∙AI 일자리에서 학위를 요구하는 구인 공고는 2018년 36%에서 2023년 약 31%로 감소해, 정규교육보다 기술과 경험을 중시하는 방향으로 채용 정책이 전환되었음을 확인 가능n연구진에 따르면 전체 노동시장에서는 고학력 근로자일수록 임금 프리미엄을 받지만, AI 일자리에서는 기술 자체가 임금 프리미엄으로 작용∙연구진의 분석 결과, AI 일자리에서는 공식 학위 요건이 없는 직책도 학사나 박사학위를 보유한 직책과 비슷한 수준의 높은 임금을 받아 정식 교육에 대한 임금 프리미엄이 거의 사라졌음을 시사∙AI 기술은 그 자체로 평균 23%의 임금 프리미엄이 있으며, 이는 박사학위(33%)에 이어 두 번째로 높고 석사학위(13%)를 능가하는 수치로 확인n연구진은 영국 노동시장에서 AI 전문가들은 학위가 아니라 실무기술과 산업별 노하우를 바탕으로 보상을 받는다며, 산업 수요에 따른 유연한 교육 프로그램과 정규교육 이외의 경로로 습득한 기술에 대한 자격 인증의 필요성을 강조∙연구진은 고용주와 영국 정부에 AI 분야의 인력 확보를 위해 견습 제도, 실무 교육, 온라인 공개 강좌(MOOC), 직업 교육과 훈련, 부트캠프 등 정규교육 이외의 다양한 대안을 주류화할 것을 권고출처 | Skills or degree? The rise of skill-based hiring for AI and green jobs, Technological Forecasting and Social Change.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24'}\n","2025.02.26\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25'}\n","정책･법제기업･산업기술･연구인력･교육\n","23\n","스탠포드 HAI, AI+교육 서밋에서 AI가 교육에 미치는 영향 논의n스탠포드 HAI가 개최한 제3회 AI+교육 서밋의 참가자들은 교육 분야에서 윤리적이고 책임 있는 방식으로 AI를 활용하기 위한 논의를 진행n참가자들은 교육 분야의 AI에 대한 신중한 설계와 윤리적 고려를 강조하는 한편, 교육용 AI의 설계와 구현 프로세스에 교사와 학생, 학부모, 정책 입안자의 참여가 필요하다는데 합의  \n","KEY Contents\n","£서밋 참가자들, 교육용 AI 도구의 신중한 설계와 윤리적 고려 강조n미국 스탠포드 인간중심AI연구소(Stanford HAI)는 AI 연구자, 교육자, 개발자, 정책 입안자들이 모여 AI가 교육과 학습에 미치는 영향을 논의하고자 2025년 2월 25일 제3회 스탠포드 AI+교육 서밋을 개최∙참가자들은 교육과 관련된 AI 연구를 소개하는 한편, 교육 분야에서 윤리적이고 책임 있는 방식으로 AI를 활용하기 위한 논의를 진행n스탠포드⼤ 소속의 연구자들은 AI가 교육에 미치는 영향 및 AI를 활용해 교육 연구를 촉진하는 방법을 논의∙교육학과의 빅터 리(Victor Lee) 교수는 AI 리터러시와 관련해 사용자와 개발자, 비평가의 관점을 모두 포괄하는 공통 언어와 프레임워크의 필요성을 강조∙인간생물학 분야의 마이클 프랭크(Michael Frank) 교수는 헤드 카메라로 수집한 아동의 언어 입력과 처리 데이터를 이용해 AI 모델을 훈련함으로써 아동의 학습 및 발달 방식을 파악하는 연구 프로젝트 ‘베이비뷰(Babyview)’를 소개n교육 관련 비영리단체와 교육자들은 학습을 혁신할 잠재력을 가진 AI에 대한 신중한 설계와 윤리적 고려의 필요성을 강조∙교육 비영리단체 InnovateEDU의 에린 모트(Erin Mote) CEO는 교육 분야의 AI 도입 시 프라이버시의 중요성을 강조하며, 학생 정보와 프라이버시에 대한 보호 책임을 명확히 하고 교육용 AI 도구의 편향을 완화하는 데 집중해야 한다고 발언∙캐서린 트루잇(Catherine Truitt) 전 노스캐롤라이나주 교육감은 AI 사용에 대한 주 차원의 지침이 필요하고 강조하며, 현재까지 교육 분야의 AI 사용 지침을 마련한 주가 26개 주에 불과하다고 언급n참가자들은 서밋 전반에 걸쳐 AI가 인간관계를 보완하되 대체해서는 안 되며, AI 도구의 설계와 접근성 고려 시 형평성을 최우선시해야 한다고 강조∙참가자들은 이를 위해 교사, 학생, 학부모, 정책 입안자가 AI 기술의 설계와 구현 프로세스에 참여해야 한다는 데 대체로 동의∙뉴욕시 공립학교의 디지털 학습 이니셔티브 책임자인 타라 카로자(Tara Carroza)도 미국의 교육 분야가 세계적 경쟁력을 갖추려면 지역사회의 모든 구성원이 AI 도입 과정에 참여해야 한다고 발언출처 | Stanford HAI, AI+Education Summit: The Future is Already Here, 2025.02.27.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 25, 'page_label': '26'}\n","SPRi AI Brief2025년 4월호\n","24\n","영국 정부의 AI 저작권 규제 완화 기조에 창작자들의 반발 격화n영국 정부가 2024년 12월 AI 기업들이 저작권자의 허가 없이 저작물을 사용할 수 있도록 저작권법 변경을 제안하면서 창작 산업계의 반발이 지속n유명 배우와 가수를 포함한 창작 산업계의 반발이 격화되자 영국 정부는 2025년 말 발표 예정인 최종 저작권 정책에서 저작권자를 보호할 수 있는 새로운 방안을 모색\n","KEY Contents\n","£영국 창작 산업계, 영국 정부의 AI 저작권 규제 완화 기조에 반발 지속n영국 일간지 가디언(The Guardian)에 따르면 영국 정부가 AI 기업들이 저작권자의 허가 없이 저작물을 사용할 수 있도록 저작권 정책 변경을 추진하면서 창작자들의 반발이 지속∙케이트 부시(Kate Bush), 한스 짐머(Hans Zimmer) 등 음악가 1,000명은 2025년 2월 영국 정부의 저작권 완화 정책에 대한 항의의 표시로 빈 스튜디오나 공연장의 소음을 담은 무음 앨범을 공동 발매∙영화배우 줄리앤 무어(Julianne Moore)를 포함한 4만 명 이상의 예술가들은 “생성 AI 훈련 시 창작물의 허가 없는 사용은 저작권자의 생계에 대한 부당한 위협으로서 허용되어서는 안 된다”는 내용의 성명에 참여n영국 정부는 2024년 12월 ’저작권과 AI에 관한 협의안’을 통해 AI 기업들에 ‘텍스트 및 데이터 마이닝’에 대한 예외를 제공하여 저작물을 이용한 AI 모델 훈련을 허용하는 방안을 제안∙영국 정부는 협의안에서 저작권법을 개정해 창작자가 자신의 작품이 AI 학습에 이용되지 않도록 거부할 수 있는 ‘옵트아웃(Opt-out)*’ 권리를 행사하는 방안을 제시* 기업 등의 데이터의 수집이나 활용에 대하여 정보 소유자가 명시적으로 거부 의사를 표현하는 제도∙그러나 창작자들은 옵트아웃 방식은 해당 옵션의 존재를 모르는 창작자에게 부담을 전가하게 되며, 인터넷상의 콘텐츠 유통을 추적하기 힘들다는 점에서 도입이 어렵다고 지적하며 저작권법의 개정에 반대∙한편, 오픈AI를 비롯한 기술업계는 저작물을 사용하지 않고 AI 모델을 훈련하기는 불가능하다고 인정했으며, 기술업계를 대표하는 산업단체 TechUK는 AI와 저작권법에 대한 현재의 불확실성이 AI의 발전을 저해하고 있다고 주장£영국 정부, 2025년 말 발표할 최종 저작권 정책에서 저작권자 보호할 새로운 방안 모색n가디언에 따르면 유명인을 포함한 창작 산업계의 반발이 이어지면서 영국 정부도 2025년 말 발표 예정인 최종 저작권 정책에서 저작권자를 보호하는 새로운 방안을 모색 중인 것으로 확인∙영국 정부는 영국 AI 기업에는 옵트아웃 방식을, 미국 AI 기업에는 저작물 사용 전 창작자의 동의를 강제하는 방식도 거론∙영국 산업통상부의 조나단 레이놀즈(Jonathan Reynolds) 장관은 창작자들이 적절한 생계를 유지할 수 있어야 하며, 정부가 창작 산업계의 우려에 주목해 AI 산업 발전과 창작자 저작권 보호 간 적절한 균형을 맞춰야 한다고 강조출처 | The Guardian, Why are creatives fighting UK government AI proposals on copyright?, 2025.02.25. The Guardian, UK ministers consider changing AI plans to protect creative industries, 2025.02.25.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 25, 'page_label': '26'}\n","Gov.uk, Copyright and Artificial Intelligence, 2024.12.17.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 26, 'page_label': '27'}\n","정책･법제기업･산업기술･연구인력･교육\n","25\n","IBM CEO, 가까운 미래에 AI가 프로그래머를 대체할 가능성은 희박하다고 예측nIBM의 아르빈드 크리슈나 CEO는 6개월 안에 코드의 90%가 AI에 의해 작성될 것이라는 앤스로픽 CEO의 주장에 반박하면서, AI가 인간 프로그래머를 대체할 가능성은 희박하다고 예상n크리슈나 CEO는 AI는 궁극적으로 프로그래머의 생산성을 높일 것으로 예상하는 한편, 기존의 지식으로 학습한 AI가 인공 일반지능(AGI)로 발전할 가능성에는 회의를 표시\n","KEY Contents\n","£크리슈나 CEO, AI로 작성되는 코드는 전체의 20~30% 수준으로 예상nIT 미디어 테크크런치(TechCrunch)의 2025년 3월 11일 보도에 따르면, 아르빈드 크리슈나(Arvind Krishna) IBM CEO는 AI가 짧은 기간 안에 프로그래머를 대체할 가능성은 희박하다고 예측∙다리오 아모데이(Dario Amodei) 앤스로픽 CEO는 2025년 3월 11일 미국 외교협회의 행사에 참석해 “3~6개월 안에 코드의 90%가 AI에 의해 작성되고, 12개월 이내에는 거의 모든 코드가 AI에 의해 생성될 것”이라고 발언∙크리슈나 CEO는 이러한 예측을 반박하면서, 복잡한 활용 사례를 고려하면 AI가 코드의 약 20~30% 수준만 작성할 수 있을 것으로 예상n크리슈나 CEO는 일부 AI 전문가들의 추측대로 AI가 인간 프로그래머를 대체하는 것이 아니라, 궁극적으로 프로그래머의 생산성을 높일 것으로 전망∙그는 같은 수의 인원으로 30% 더 많은 코드를 작성할 수 있다면 코드 작성이 더욱 늘어날 것이라며, 역사적으로 볼 때 가장 생산적인 기업이 더 많은 제품을 생산해 시장 점유율을 높이게 된다고 강조∙이는 AI 기술로 대체할 수 있는 운영 지원부서에 대한 채용 중단 계획을 밝힌 2023년의 발언과 대조되는 것으로, 현재 IBM은 코딩 보조 도구를 포함한 다양한 AI 기반 제품과 서비스를 판매∙그는 AI가 노동자를 대체할 것이라는 주장은 과거 계산기와 포토샵이 수학자와 예술가를 대체할 것이라는 주장과 다름없다며, AI는 궁극적으로 인간을 돕는 도구라고 설명£크리슈나 CEO, 딥시크 사례와 같은 기술 발전으로 AI의 에너지 사용량 대폭 감소 전망n한편, 크리슈나 CEO는 향후 AI가 딥시크(DeepSeek)가 보여준 것과 같은 기술 발전에 힘입어 현재 사용하는 에너지의 1% 미만을 사용하게 될 것으로 예상∙그는 오픈AI o1 같은 추론 모델은 막대한 컴퓨팅 자원을 사용해 에너지 집약적이나, 딥시크 이후 등장한 기술 발전 사례를 언급하며, 대규모 모델의 필요성에 의문이 제기되고 있다고 설명n그러나 크리슈나 CEO는 기존에 만들어진 지식과 문헌, 이미지 등에서 학습한 AI는 앞으로 일어날 일을 예측하지는 못한다면서, 인공 일반지능(AGI)에 대한 회의를 표시∙이는 향후 몇 년 안에 초지능 AI가 실현되어 혁신을 대폭 앞당길 것이라는 샘 알트먼(Sam Altman) 오픈AI CEO의 주장과 대조되며, 크리슈나 CEO는 양자 컴퓨팅이 과학적 발견을 촉진하는 핵심 기술이 될 것으로 예상출처 | TechCrunch, IBM’s CEO doesn’t think AI will replace programmers anytime soon, 2025.03.11.\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 27, 'page_label': '28'}\n","26\n","주요행사일정월기간행사명장소홈페이지1월7~10일(CES 2025) The International Consumer Electronics Show미국, 라스베이거스www.ces.tech2월5~6일AI & Big Data Expo Global 2025영국, 런던www.ai-expo.net/global27~4일(AAAI 2025) Association for the Advancement of Artificial Intelligence Conference미국, 필라델피아aaai.org/conference/aaai/aaai-253월17~21일NVIDIA GTC 2025미국, 산호세(온라인 병행)www.nvidia.com/ko-kr/gtc26~27일Chief Data & Analytics Officers캐나다, 토론토cdao-canada.coriniumintelligence.com4월15~16일World Summit AI Americas캐나다, 몬트리올americas.worldsummit.ai24~26일 월드 IT 쇼 2025서울, 강남www.worlditshow.co.kr29일 LlamaCon 2025미국, 멘로파크www.llama.com/events/llamacon/signup29~30일Generative AI Summit미국, 산타클라라world.aiacceleratorinstitute.com/location/siliconvalley5월5~7일(IEEE CAI 2025) IEEE Conference on Artificial Intelligence미국, 산타클라라cai.ieee.org/20256~8일Microsoft 365 Conference미국, 라스베이거스m365conf.com14일Rise of AI Conference독일, 베를린(온라인 병행)riseof.ai/conference-202519~22일Microsoft Build 2025미국, 시애틀build.microsoft.com/en-US/home20~21일Google I/O 2025미국, 마운틴뷰io.google/20256월4~5일AI & Big Data Expo North America 2025미국, 산타클라라www.ai-expo.net/northamerica9~13일 WWDC25미국, 쿠퍼티노developer.apple.com11~15일(CVPR 2025) The IEEE / CVF Computer Vision and Pattern Recognition Conference 미국, 네슈빌cvpr.thecvf.com11~12일AI SUMMIT LONDON영국, 런던london.theaisummit.com11~13일(STK 2025) 스마트테크 코리아서울, 강남smarttechkorea.com18~19일AI World Congress 2025영국, 런던aiconference.london18~20일(MVEX 2025) 2025 메타버스 엑스포서울, 강남metavexpo.com7월8~11일AI for Good Global Summit 2025스위스, 제네바aiforgood.itu.int13~19일 ICML 2025캐나다, 밴쿠버icml.cc25~27일(AICSIP 2025) 2025 IEEE 7th International Conference on Artificial Intelligence, Computer Science and Information Processing중국, 항저우www.aicsconf.cn27~1일(ACL 2025) the Association for Computational Linguistics오스트리아, 빈2025.aclweb.org8월11~13일(Ai4 2025) the Forefront of AI Innovation미국, 라스베이거스ai4.io/vegas9월9~11일AI Infra Summit 2025미국, 산타클라라www.ai-infra-summit.com17~18일The AI Conference미국, 샌프란시스코aiconference.com17~18일 Meta Connect미국, 멘로파크www.meta.com/connect24~25일AI & Big Data Expo EUROPE 2025네덜란드, 암스테르담www.ai-expo.net/europe10월8~9일 World Summit AI네덜란드, 암스테르담worldsummit.ai11월13~14일AI and Machine Learning Conference 2025싱가포르pubscholars.org/ai-and-machine-learning-conference14~15일2025 ICT 산업전망컨퍼런스서울, 서초ictconference.kr17~21일 Microsoft Ignite미국, 샌프란시스코ignite.microsoft.com12월2~7일 NeurIPS 2025미국, 샌디에이고neurips.cc3~5일(소프트웨이브 2025) 10회 대한민국 소프트웨어 대전서울, 강남www.k-softwave.com10~11일AI Summit New York미국, 뉴욕newyork.theaisummit.com\n","----------------------------------------------------------------------------------------------------\n","{'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-04-04T07:23:56+09:00', 'author': 'dj', 'moddate': '2025-04-04T07:23:56+09:00', 'pdfversion': '1.4', 'source': 'SPRi AI Brief_4월호_산업동향_250407_F.pdf', 'total_pages': 29, 'page': 28, 'page_label': '29'}\n","홈페이지 : https://spri.kr보고서와 관련된 문의는 AI정책연구실(hs.lee@spri.kr, 031-739-7333)로 연락주시기 바랍니다.경기도 성남시 분당구 대왕판교로 712번길 22 글로벌 R&D 연구동(B) 4층22, Daewangpangyo-ro 712beon-gil, Bundang-gu, Seongnam-si, Gyeonggi-do, Republic of Korea, 13488\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# **3. 벡터 리트리버**\n","벡터 리트리버(Vector Retriever)는 사용자의 질의(Query)를 임베딩 벡터로 변환한 뒤, 벡터 데이터베이스에 저장된 청크(Chunk) 벡터들과의 유사도를 계산하여 가장 관련성 높은 결과를 찾아주는 구성 요소입니다. 즉, “검색기” 역할을 하는데, 단순히 키워드 일치를 찾는 것이 아니라 의미적 유사성을 기반으로 정보를 불러옵니다. 이를 위해 코사인 유사도, 내적(dot product), 유클리드 거리 등의 수학적 방법을 활용하며, 검색된 결과는 LLM과 결합되어 RAG(Retrieval-Augmented Generation) 같은 구조에서 모델의 답변 품질을 높이는 데 쓰입니다. 쉽게 말해, 벡터 리트리버는 “의미를 이해하는 검색 엔진”이라고 할 수 있습니다."],"metadata":{"id":"puMSh-qTEbyt"}},{"cell_type":"markdown","source":["from_documents\n","\n","* documents (List[Document]): 벡터 저장소에 추가할 문서 리스트\n","* embedding (Optional[Embeddings]): 임베딩 함수. 기본값은 None\n","* ids (Optional[List[str]]): 문서 ID 리스트. 기본값은 None\n","collection_name (str): 생성할 컬렉션 이름.\n","* persist_directory (Optional[str]): 컬렉션을 저장할 디렉토리. 기본값은 None\n","* client_settings (Optional[chromadb.config.Settings]): Chroma 클라이언트 설정\n","* client (Optional[chromadb.Client]): Chroma 클라이언트 인스턴스\n","* collection_metadata (Optional[Dict]): 컬렉션 구성 정보. 기본값은 None"],"metadata":{"id":"DzUQtLIQEfs4"}},{"cell_type":"code","source":["!pip install langchain_chroma\n","\n","!pip install -U langchain langchain-chroma langchain-openai chromadb\n","\n","!pip install -U \"opentelemetry-api==1.26.0\" \\\n","              \"opentelemetry-sdk==1.26.0\" \\\n","              \"opentelemetry-exporter-otlp-proto-grpc==1.26.0\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"yOmv8W86G3oG","executionInfo":{"status":"ok","timestamp":1758070198886,"user_tz":-540,"elapsed":30442,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"e1e885c1-24c4-4755-cb95-6f6f9d142ec0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain_chroma in /usr/local/lib/python3.12/dist-packages (0.2.6)\n","Requirement already satisfied: langchain-core>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain_chroma) (0.3.76)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain_chroma) (2.0.2)\n","Requirement already satisfied: chromadb>=1.0.20 in /usr/local/lib/python3.12/dist-packages (from langchain_chroma) (1.1.0)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.3.0)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (2.11.7)\n","Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.4.2)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.35.0)\n","Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (5.4.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.15.0)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.22.1)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.37.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.37.0)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.37.0)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.22.0)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.67.1)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.74.0)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.3.0)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.17.4)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (33.1.0)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (8.5.0)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (6.0.2)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (5.2.0)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (3.11.3)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.25.1)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.76->langchain_chroma) (0.4.27)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.76->langchain_chroma) (1.33)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.76->langchain_chroma) (25.0)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (1.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (4.10.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.76->langchain_chroma) (3.0.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.27.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.9.0.post0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.38.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.32.5)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.3.1)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.5.0)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.10)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (0.24.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.13.3)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain_chroma) (8.7.0)\n","Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.70.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.37.0)\n","Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.37.0)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.20->langchain_chroma) (0.58b0)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (2.2.1)\n","Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (1.9.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (0.4.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (2.19.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (0.34.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (1.5.4)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.1)\n","Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.21.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (4.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (3.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (1.1.9)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain_chroma) (3.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (0.1.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.4.3)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.3.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.6.1)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n","Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.12/dist-packages (0.2.6)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.33)\n","Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.27)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (2.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.107.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n","Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n","Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n","Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n","Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n","Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n","Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Collecting opentelemetry-api==1.26.0\n","  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting opentelemetry-sdk==1.26.0\n","  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api==1.26.0)\n","  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n","Collecting importlib-metadata<=8.0.0,>=6.0 (from opentelemetry-api==1.26.0)\n","  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk==1.26.0)\n","  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk==1.26.0) (4.15.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0) (1.70.0)\n","Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0) (1.74.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0)\n","  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting protobuf<5.0,>=3.19 (from opentelemetry-proto==1.26.0->opentelemetry-exporter-otlp-proto-grpc==1.26.0)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2.6->opentelemetry-api==1.26.0) (1.17.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api==1.26.0) (3.23.0)\n","Downloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n","Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n","Downloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n","Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf, importlib-metadata, deprecated, opentelemetry-proto, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 8.7.0\n","    Uninstalling importlib_metadata-8.7.0:\n","      Successfully uninstalled importlib_metadata-8.7.0\n","  Attempting uninstall: opentelemetry-proto\n","    Found existing installation: opentelemetry-proto 1.37.0\n","    Uninstalling opentelemetry-proto-1.37.0:\n","      Successfully uninstalled opentelemetry-proto-1.37.0\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.37.0\n","    Uninstalling opentelemetry-api-1.37.0:\n","      Successfully uninstalled opentelemetry-api-1.37.0\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n","    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n","    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.37.0\n","    Uninstalling opentelemetry-sdk-1.37.0:\n","      Successfully uninstalled opentelemetry-sdk-1.37.0\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\n","    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.37.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.37.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.37.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","google-adk 1.13.0 requires opentelemetry-api<2.0.0,>=1.31.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n","google-adk 1.13.0 requires opentelemetry-sdk<2.0.0,>=1.31.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed deprecated-1.2.18 importlib-metadata-8.0.0 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 protobuf-4.25.8\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","importlib_metadata","opentelemetry"]},"id":"6e052bac927e42adaf16f60bc45b6a37"}},"metadata":{}}]},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","\n","vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())"],"metadata":{"id":"Z2Y8Xwd3HCA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# query = \"카나나의 테크니컬 리포트는 어떤 내용인가요?\"\n","# query = \"에이전트 SDK는 어떤 기능을 제공하나요?\"\n","query = \"딥마인드가 발표한 로봇AI 모델은?\"\n","\n","results = vectorstore.similarity_search(query, k=1)"],"metadata":{"id":"VwBa6S4qHFDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(results[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HroCbqOUHF_u","executionInfo":{"status":"ok","timestamp":1758070208592,"user_tz":-540,"elapsed":9,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"df5c2752-65cc-4bd3-83d0-c17d79f30787"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SPRi AI Brief2025년 4월호\n","16\n","구글 딥마인드, 로봇 특화 AI 모델 ‘제미나이 로보틱스’ 개발n구글 딥마인드는 제미나이 2.0 기반의 시각-언어-행동 모델로 로봇 직접 제어를 위한 물리적 동작이 출력으로 추가된 로봇 특화 AI 모델 ‘제미나이 로보틱스’를 공개n구글 딥마인드는 물리적 세계에 대한 이해 능력을 개선해 로봇 제어에 필요한 모든 추론 단계를 즉각 수행할 수 있는 비전-언어 모델 ‘제미나이 로보틱스-ER’도 공개\n","KEY Contents\n","£‘제미나이 로보틱스’, 학습 과정에서 접하지 못한 작업도 즉각 수행하는 범용성 보유n구글 딥마인드(Google Deepmind)가 2025년 3월 12일 로봇 개발에 특화된 AI 모델 ‘제미나이 로보틱스(Gemini Robotics)’를 공개∙제미나이 2.0을 기반으로 구축된 제미나이 로보틱스는 첨단 시각-언어-행동(VLA) 모델로, 로봇을 직접 제어하기 위해 물리적 행동을 새로운 출력 형태로 추가했으며, 학습 과정에서 접하지 못한 새로운 물체나 환경에서도 기존에 학습한 지식을 응용해 다양한 작업을 즉각 수행하는 범용성을 보유* Vision-Language-Action: 시각, 언어, 행동을 통합적으로 이해하고 처리하는 AI 모델∙구글 딥마인드는 종합적인 범용성 벤치마크에서 제미나이 로보틱스가 다른 첨단 시각-언어-행동 모델보다 평균 두 배 이상의 성능 향상을 보였다고 보고*n구글 딥마인드에 따르면 제미나이 로보틱스는 제미나이의 언어 이해 능력을 바탕으로 일상적 대화체의 명령을 이해하고 다양한 언어로 답변할 수 있으며, 복잡한 다단계 작업도 수행 가능∙주변 환경을 지속적으로 모니터링하여 환경이나 명령의 변화를 감지하고 즉시 행동을 수정할 수 있어, 다양한 환경에서 로봇과 협력할 수 있도록 지원∙종이접기처럼 섬세한 운동 능력과 정밀한 조작을 요구하는 작업을 수행할 수 있으며, 로봇 유형별로 쉽게 적응할 수 있도록 설계되어 많은 연구실에서 사용되는 양팔 로봇 플랫폼뿐 아니라 휴머노이드 로봇과 같은 복잡한 형태에도 특화 가능£로봇 제어에 필요한 추론 수행 능력을 강화한 ‘제미나이 로보틱스-ER’도 공개n구글 딥마인드는 제미나이 로보틱스와 함께 첨단 시각-언어 모델 ‘제미나이 로보틱스-ER (Embodied Reasoning)’도 공개∙제미나이 로보틱스-ER은 물리적 세계에 대한 이해 능력을 향상한 모델로서, 3D 인식이나 상태 추정, 공간 이해, 계획과 코드 생성 등 로봇 제어에 필요한 모든 추론 단계를 즉각 수행 가능∙향상된 공간 추론 능력과 제미나이의 코딩 능력을 결합해 즉각적으로 새로운 기능을 구현할 수 있으며, 일례로 커피잔을 보여주면 잔을 들어올리기에 적합한 그립 방식과 안전한 접근 경로를 직관적으로 파악∙모델 안전성을 위해 충돌 회피와 같은 기본적인 안전 제어장치와 상호작용할 수 있으며, 제미나이의 안전 기능을 바탕으로 주어진 시나리오 안에서 안전한 동작을 판단하고, 질문에 적절한 답변을 생성하도록 설계 출처 | Google Deepmind, Gemini Robotics brings AI into the physical world, 2025.03.12.\n"]}]},{"cell_type":"code","source":["# 벡터스토어를 검색기로 바꿔줌. 가장 유사한 문서 1개만 찾음\n","vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n","relevant_doc = vector_retriever.invoke(query)\n","print(relevant_doc)"],"metadata":{"id":"nigZy0EBHGtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(relevant_doc[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQg2pjvYHJUv","executionInfo":{"status":"ok","timestamp":1758070208800,"user_tz":-540,"elapsed":15,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"744fd721-2992-43b6-db51-9cc6a8e985e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SPRi AI Brief2025년 4월호\n","16\n","구글 딥마인드, 로봇 특화 AI 모델 ‘제미나이 로보틱스’ 개발n구글 딥마인드는 제미나이 2.0 기반의 시각-언어-행동 모델로 로봇 직접 제어를 위한 물리적 동작이 출력으로 추가된 로봇 특화 AI 모델 ‘제미나이 로보틱스’를 공개n구글 딥마인드는 물리적 세계에 대한 이해 능력을 개선해 로봇 제어에 필요한 모든 추론 단계를 즉각 수행할 수 있는 비전-언어 모델 ‘제미나이 로보틱스-ER’도 공개\n","KEY Contents\n","£‘제미나이 로보틱스’, 학습 과정에서 접하지 못한 작업도 즉각 수행하는 범용성 보유n구글 딥마인드(Google Deepmind)가 2025년 3월 12일 로봇 개발에 특화된 AI 모델 ‘제미나이 로보틱스(Gemini Robotics)’를 공개∙제미나이 2.0을 기반으로 구축된 제미나이 로보틱스는 첨단 시각-언어-행동(VLA) 모델로, 로봇을 직접 제어하기 위해 물리적 행동을 새로운 출력 형태로 추가했으며, 학습 과정에서 접하지 못한 새로운 물체나 환경에서도 기존에 학습한 지식을 응용해 다양한 작업을 즉각 수행하는 범용성을 보유* Vision-Language-Action: 시각, 언어, 행동을 통합적으로 이해하고 처리하는 AI 모델∙구글 딥마인드는 종합적인 범용성 벤치마크에서 제미나이 로보틱스가 다른 첨단 시각-언어-행동 모델보다 평균 두 배 이상의 성능 향상을 보였다고 보고*n구글 딥마인드에 따르면 제미나이 로보틱스는 제미나이의 언어 이해 능력을 바탕으로 일상적 대화체의 명령을 이해하고 다양한 언어로 답변할 수 있으며, 복잡한 다단계 작업도 수행 가능∙주변 환경을 지속적으로 모니터링하여 환경이나 명령의 변화를 감지하고 즉시 행동을 수정할 수 있어, 다양한 환경에서 로봇과 협력할 수 있도록 지원∙종이접기처럼 섬세한 운동 능력과 정밀한 조작을 요구하는 작업을 수행할 수 있으며, 로봇 유형별로 쉽게 적응할 수 있도록 설계되어 많은 연구실에서 사용되는 양팔 로봇 플랫폼뿐 아니라 휴머노이드 로봇과 같은 복잡한 형태에도 특화 가능£로봇 제어에 필요한 추론 수행 능력을 강화한 ‘제미나이 로보틱스-ER’도 공개n구글 딥마인드는 제미나이 로보틱스와 함께 첨단 시각-언어 모델 ‘제미나이 로보틱스-ER (Embodied Reasoning)’도 공개∙제미나이 로보틱스-ER은 물리적 세계에 대한 이해 능력을 향상한 모델로서, 3D 인식이나 상태 추정, 공간 이해, 계획과 코드 생성 등 로봇 제어에 필요한 모든 추론 단계를 즉각 수행 가능∙향상된 공간 추론 능력과 제미나이의 코딩 능력을 결합해 즉각적으로 새로운 기능을 구현할 수 있으며, 일례로 커피잔을 보여주면 잔을 들어올리기에 적합한 그립 방식과 안전한 접근 경로를 직관적으로 파악∙모델 안전성을 위해 충돌 회피와 같은 기본적인 안전 제어장치와 상호작용할 수 있으며, 제미나이의 안전 기능을 바탕으로 주어진 시나리오 안에서 안전한 동작을 판단하고, 질문에 적절한 답변을 생성하도록 설계 출처 | Google Deepmind, Gemini Robotics brings AI into the physical world, 2025.03.12.\n"]}]},{"cell_type":"markdown","source":["**앙상블 리트리버**\n","\n","앙상블 리트리버(Ensemble Retriever)는 하나의 검색 방식에만 의존하지 않고, 여러 종류의 리트리버를 조합해 더 정확하고 풍부한 검색 결과를 제공하는 방법입니다. 예를 들어 키워드 기반의 전통적 BM25 리트리버와 의미 기반의 벡터 리트리버를 함께 사용하면, 단어가 정확히 일치하는 문서뿐 아니라 의미적으로 관련 있는 문서도 함께 찾아낼 수 있습니다. 이렇게 서로 다른 리트리버의 강점을 결합하면 검색 누락을 줄이고, 다양한 관점에서 문서를 확보할 수 있어 RAG(Retrieval-Augmented Generation) 구조에서 더욱 신뢰도 높은 응답을 생성하는 데 유용합니다."],"metadata":{"id":"OAbaTeeNHKgv"}},{"cell_type":"markdown","source":["**BM25 리트리버**\n","\n","BM25 리트리버는 전통적인 정보 검색 기법 중 하나로, 사용자의 질의(Query)와 문서 간의 키워드 일치 정도를 계산해 관련성이 높은 문서를 찾아주는 방식입니다. 기본적으로 단어 빈도(Term Frequency), 역문서 빈도(Inverse Document Frequency), 그리고 문서 길이를 고려해 점수를 매기며, 특정 단어가 질의에 많이 등장하거나 드문 단어일수록 가중치를 높게 주어 검색 정확도를 높입니다. 벡터 리트리버처럼 의미적 유사성을 직접 파악하지는 못하지만, 빠르고 해석 가능한 결과를 제공하기 때문에 대규모 문서 검색이나 키워드 중심 검색에서 여전히 많이 활용되며, 종종 벡터 리트리버와 결합해 앙상블 리트리버로 사용됩니다."],"metadata":{"id":"YN0wCoZbHMkO"}},{"cell_type":"code","source":["!pip install rank_bm25"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"fOWESmg6HX2x","executionInfo":{"status":"ok","timestamp":1758070232536,"user_tz":-540,"elapsed":7684,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"fff0f268-b2ee-43de-f6ef-a245fe89c557"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rank_bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n","Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Installing collected packages: rank_bm25\n","Successfully installed rank_bm25-0.2.2\n"]}]},{"cell_type":"code","source":["from langchain.retrievers import BM25Retriever, EnsembleRetriever\n","\n","bm25_retriever = BM25Retriever.from_documents(\n","    docs,\n",")\n","bm25_retriever.k = 1"],"metadata":{"id":"ZTivVJCGJs3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ensemble_retriever = EnsembleRetriever(\n","    # BM25에 더 높은 비중(70%)을 주고, 임베딩 검색은 보조(30%)로 섞음\n","    retrievers=[bm25_retriever, vector_retriever],\n","    weights=[0.7, 0.3],\n",")"],"metadata":{"id":"8OUbhkocJuy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"깊이 업스케일링 (Depth UP-Scaling)\""],"metadata":{"id":"ZkIFbwgrJv3P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ensemble_result = ensemble_retriever.invoke(query)\n","bm25_result = bm25_retriever.invoke(query)\n","vector_result = vector_retriever.invoke(query)\n","\n","print(\"[Ensemble Retriever]\")\n","for doc in ensemble_result:\n","    print(f\"Content: {doc.page_content}\")\n","    print()\n","\n","print(\"[BM25 Retriever]\")\n","for doc in bm25_result:\n","    print(f\"Content: {doc.page_content}\")\n","    print()\n","\n","print(\"[Vector Retriever]\")\n","for doc in vector_result:\n","    print(f\"Content: {doc.page_content}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6EojpmtJwzD","executionInfo":{"status":"ok","timestamp":1758071355157,"user_tz":-540,"elapsed":715,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"c55298b3-7c80-4c6b-f0c5-f2e1026d720d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Ensemble Retriever]\n","Content: 홈페이지 : https://spri.kr보고서와 관련된 문의는 AI정책연구실(hs.lee@spri.kr, 031-739-7333)로 연락주시기 바랍니다.경기도 성남시 분당구 대왕판교로 712번길 22 글로벌 R&D 연구동(B) 4층22, Daewangpangyo-ro 712beon-gil, Bundang-gu, Seongnam-si, Gyeonggi-do, Republic of Korea, 13488\n","\n","Content: 2025년4월호인공지능 산업의 최신 동향\n","\n","[BM25 Retriever]\n","Content: 홈페이지 : https://spri.kr보고서와 관련된 문의는 AI정책연구실(hs.lee@spri.kr, 031-739-7333)로 연락주시기 바랍니다.경기도 성남시 분당구 대왕판교로 712번길 22 글로벌 R&D 연구동(B) 4층22, Daewangpangyo-ro 712beon-gil, Bundang-gu, Seongnam-si, Gyeonggi-do, Republic of Korea, 13488\n","\n","[Vector Retriever]\n","Content: 2025년4월호인공지능 산업의 최신 동향\n","\n"]}]},{"cell_type":"code","source":["!pip install langgraph"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"XvWLLwEeJx80","executionInfo":{"status":"ok","timestamp":1758071365440,"user_tz":-540,"elapsed":8052,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"9c10895a-9c5c-4a63-fc4d-26d0a56815e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langgraph\n","  Downloading langgraph-0.6.7-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.76)\n","Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n","  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n","Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n","  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n","Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n","  Downloading langgraph_sdk-0.2.7-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.7)\n","Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.27)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n","Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n","  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n","Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n","Downloading langgraph-0.6.7-py3-none-any.whl (153 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n","Downloading langgraph_sdk-0.2.7-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n","Successfully installed langgraph-0.6.7 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.7 ormsgpack-1.10.0\n"]}]},{"cell_type":"code","source":["from langgraph.graph import StateGraph, MessagesState\n","\n","class State(MessagesState):\n","    context: str\n","\n","graph_builder = StateGraph(State)"],"metadata":{"id":"-cWwBlQKJzBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","\n","def retriever(state: State):\n","    \"\"\"\n","    Retrieve the relevant document and return the content.\n","    \"\"\"\n","    print(\"##### RETRIEVER #####\")\n","    query = state[\"messages\"][0].content\n","    ensemble_result = ensemble_retriever.invoke(query)\n","\n","    content = ensemble_result[0].page_content\n","    print(\"[CONTEXT]\\n\", content)\n","\n","    return {\"context\" : content, \"messages\": [HumanMessage(content=content)]}"],"metadata":{"id":"4hnzo_ySJ0EG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **4. 랭체인 허브**\n","[랭체인 허브(LangChain Hub)](https://smith.langchain.com/hub)는 개발자와 연구자들이 프롬프트(Prompt), 체인(Chain), 에이전트(Agent) 같은 LLM 관련 리소스를 공유하고 재사용할 수 있도록 만든 오픈 플랫폼입니다. 사용자는 자신이 만든 프롬프트 템플릿을 업로드해 다른 사람과 공유할 수 있고, 다른 사람이 만든 검증된 프롬프트를 hub.pull() 같은 방식으로 손쉽게 불러와 활용할 수 있습니다. 이를 통해 매번 새롭게 프롬프트를 설계할 필요 없이 빠르게 실험하고 협업할 수 있으며, 다양한 검색 기능을 제공해 특정 목적(Q&A, 요약, RAG 등)에 맞는 템플릿을 쉽게 찾을 수 있습니다. 결국 랭체인 허브는 LLM 애플리케이션 개발을 가속화하는 중앙 저장소 역할을 합니다."],"metadata":{"id":"8CMLTekqJ1Xi"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain import hub\n","\n","llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)  # temperature=0: 일관적인 답변\n","\n","def answer(state: State):\n","    \"\"\"\n","    Answer the question based on the retrieved document.\n","    \"\"\"\n","    print(\"##### ANSWER #####\")\n","    query = state[\"messages\"][0].content\n","    context = state[\"messages\"][-1].content\n","    # context = state[\"context\"]\n","\n","    # prompt = ChatPromptTemplate.from_messages(\n","    #     [\n","    #         (\"system\",\n","    #          \"\"\"\n","    #             You are an assistant for answering questions based on retrieved document context.\n","    #             Answer in Korean.\n","\n","    #             Context: {context}\"\"\"\n","    #          ),\n","    #         (\"human\", \"{question}\"),\n","    #     ]\n","    # )\n","    prompt = hub.pull(\"rlm/rag-prompt\")\n","    # You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n","    # Question: {question}\n","    # Context: {context}\n","    # Answer:\n","\n","    response = llm.invoke(\n","        prompt.format_messages(context=context, question=query)\n","    )\n","\n","    return {\"messages\": [response]}"],"metadata":{"id":"tvwqZKJ5J4q4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hub.pull(\"rlm/rag-prompt\").pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1tAdCRZQEHM","executionInfo":{"status":"ok","timestamp":1758072246457,"user_tz":-540,"elapsed":170,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"c57e59b3-0037-450e-a778-a60c6b4cfef0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n","Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n","Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n","Answer:\n"]}]},{"cell_type":"code","source":["from langgraph.graph import START, END\n","\n","graph_builder.add_sequence([retriever, answer])\n","graph_builder.add_edge(START, \"retriever\")\n","graph_builder.add_edge(\"answer\", END)\n","graph = graph_builder.compile()\n","graph"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"2sjANtb3SleV","executionInfo":{"status":"error","timestamp":1758072580572,"user_tz":-540,"elapsed":11,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"94a4882a-6f38-47e1-9035-6af88b6b96a5"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Node names must be unique: node with the name 'retriever' already exists. If you need to use two different runnables/callables with the same name (for example, using `lambda`), please provide them as tuples (name, runnable/callable).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1196587796.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retriever\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36madd_sequence\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    686\u001b[0m                     \u001b[0;34mf\"Node names must be unique: node with the name '{name}' already exists. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                     \u001b[0;34m\"If you need to use two different runnables/callables with the same name (for example, using `lambda`), please provide them as tuples (name, runnable/callable).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Node names must be unique: node with the name 'retriever' already exists. If you need to use two different runnables/callables with the same name (for example, using `lambda`), please provide them as tuples (name, runnable/callable)."]}]},{"cell_type":"code","source":["response = graph.invoke({\"messages\": \"카나나의 테크니컬 리포트는 어떤 내용인가요?\"})\n","\n","for mes in response['messages']:\n","    mes.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"24qYKO8oQFGb","executionInfo":{"status":"error","timestamp":1758072582589,"user_tz":-540,"elapsed":12,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"b3e588f1-d3e8-43d5-d370-abbf92a0021f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'graph' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2473193188.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"카나나의 테크니컬 리포트는 어떤 내용인가요?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"]}]},{"cell_type":"code","source":["from langchain.tools.retriever import create_retriever_tool\n","\n","# 검색기, Tool 이름, Tool 설명(언제 이 툴을 써야 할지 설명)\n","retriever_tool = create_retriever_tool(\n","    ensemble_retriever,\n","    \"retrieve_AI_brief\",\n","    \"Search and return information about AI Technology and Industry.\",\n",")\n","\n","tools = [retriever_tool]"],"metadata":{"id":"gXBXl76uQGDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langgraph.graph import StateGraph, MessagesState\n","\n","graph_builder = StateGraph(MessagesState)"],"metadata":{"id":"c7zEZ1XsQG1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langgraph.prebuilt import ToolNode, tools_condition\n","\n","tool_node = ToolNode(tools=tools)\n","graph_builder.add_node(\"retriever\", tool_node)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XSm4uziQHQl","executionInfo":{"status":"ok","timestamp":1758072474070,"user_tz":-540,"elapsed":13,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"e7c7b0ad-d913-4723-f366-99522a335f4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x7daa68407740>"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["llm_with_tools = llm.bind_tools(tools)\n","\n","def chatbot(state: State):\n","    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]} # 1) 도구 호출(tool_calls) 2) AI Message\n","\n","graph_builder.add_node(\"chatbot\", chatbot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZhq71vFQIyi","executionInfo":{"status":"ok","timestamp":1758072587600,"user_tz":-540,"elapsed":14,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"ae6f9c23-7a5f-460a-b848-dfa189ae1e6d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x7daa68407740>"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["graph_builder.add_conditional_edges(\n","    \"chatbot\",\n","    tools_condition,\n","    {\n","        \"tools\" : \"retriever\",\n","        END: END\n","    }\n",")\n","graph_builder.add_node(\"answer\", answer)\n","\n","graph_builder.add_edge(START, \"chatbot\")\n","graph_builder.add_edge(\"retriever\", \"answer\")\n","graph_builder.add_edge(\"answer\", END)\n","graph = graph_builder.compile()\n","graph"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"id":"4x_3KjMZQJ3e","executionInfo":{"status":"ok","timestamp":1758072589746,"user_tz":-540,"elapsed":388,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"1dfa3b2f-1520-4c02-d29b-22b9db80728e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langgraph.graph.state.CompiledStateGraph object at 0x7daa683507d0>"],"image/png":"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\n"},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["response = graph.invoke({\"messages\": \"카나나의 테크니컬 리포트는 어떤 내용인가요?\"})\n","\n","for mes in response['messages']:\n","    mes.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-nCwx0ZtQK3k","executionInfo":{"status":"ok","timestamp":1758072600739,"user_tz":-540,"elapsed":9199,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"bc84d5a4-aa4e-47fe-b7ef-27c789734911"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","카나나의 테크니컬 리포트는 어떤 내용인가요?\n","==================================\u001b[1m Ai Message \u001b[0m==================================\n","\n","혹시 말씀하신 “카나나(Kanana)”가 누구를 가리키는지, 그리고 어떤 기술 리포트를 의미하는지 조금만 더 구체적으로 알려주실 수 있을까요? 예를 들면:\n","- 카나나가 특정 연구소/기업/캐릭터의 이름인지\n","- 특정 리포트의 제목이나 링크가 있는지\n","\n","확실한 정보를 드리려면 추가 정보가 필요합니다. 다만 일반적으로 기술 리포트에 어떤 내용이 들어 있는지 미리 알려드리면 도움이 될 거예요.\n","\n","일반적으로 기술 리포트에 포함되는 구성 요소\n","- 요약/초록\n","- 목적과 연구 범위\n","- 배경 및 관련 연구 소개\n","- 시스템 아키텍처나 설계 개요\n","- 데이터 및 실험 설정(데이터 수집 방법, 실험 환경 등)\n","- 구현 세부사항(알고리즘, 모델 구조, 파라미터 등)\n","- 실험 결과 및 분석(성능 지표, 비교 평가)\n","- 한계 및 위험 요인\n","- 결론 및 향후 연구/개선 방향\n","- 부록(추가 그래프, 수식, 구현 세부정보)\n","- 참고문헌\n","\n","원하시면 제가 실제로 Kana나나의 특정 기술 리포트를 찾아 요약해 드릴 수 있습니다. 링크나 제목을 주시거나, 원하시는 분야(예: AI 모델 성능 평가, 시스템 아키텍처, 데이터 처리 파이프라인 등)를 알려주시면 더 구체적으로 도와드리겠습니다. 필요하시면 검색을 시도해 요약도 제공해 드릴 수 있어요.\n"]}]},{"cell_type":"code","source":["for chunk in graph.stream({\"messages\": \"안녕하세요.\"}):\n","    for node, value in chunk.items():\n","        if node:\n","            print(\"=====\", node, \"=====\")\n","        if \"messages\" in value:\n","            print(value['messages'][0].content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVhmzEkUQL3H","executionInfo":{"status":"ok","timestamp":1758072616424,"user_tz":-540,"elapsed":3908,"user":{"displayName":"김혜진","userId":"12819609804489028098"}},"outputId":"355dc2a7-2463-4a5c-e481-e2704bf61c8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["===== chatbot =====\n","안녕하세요! 반갑습니다. 무엇을 도와드릴까요?\n","\n","- AI 기술이나 산업 동향에 대해 간단히 정리해 드릴 수 있어요\n","- 특정 개념이나 용어를 설명해 드릴까요\n","- 한국어로 번역이나 글쓰기 도와드리기\n","- 코드나 기술 문제 해결에 도움 드리기\n","원하시는 주제를 말씀해 주시면 바로 도와드리겠습니다.\n"]}]},{"cell_type":"code","source":["."],"metadata":{"id":"fvVRJs00LLcC"},"execution_count":null,"outputs":[]}]}